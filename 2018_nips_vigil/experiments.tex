\section{Experiments}

We propose to further analyze MAC's generalization and transfer learning abilities by testing it on the CLEVR-CoGenT dataset. It is synthesized in the same way as CLEVR but contains two conditions: in Condition A, all cubes are gray, blue, brown, or yellow and all cylinders are red, green, purple, or cyan; in Condition B, cubes and cylinders swap color palettes. Both conditions contain spheres of all colors. We also evaluate the simplified model on this dataset for comparison.

The training procedure is layed out as follows: we train the MAC model (and simplified MAC) for 20 epochs on 90\% of the training sets of CLEVR \& CoGenT separately. We keep the remaining 10\% for validating the model at every epoch, and use the original validation sets as test sets, given that the ground truth labels for the true test sets are not available.

We implemented the models using PyTorch\footnote{\url{https://pytorch.org/}} (v0.4.0), and the code is available on GitHub\footnote{\url{https://github.com/IBM/mi-prometheus/}}. The system is based on Linux 16.04 and we used NVIDIA's GeForce GTX TITAN X. We followed the implementation details indicated in the original paper (Section A of the supplementary material), to ensure a faithful comparison.

\begin{table}[]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Model                 & Training set                & \multicolumn{1}{l|}{Training wall time} & Training statistics          & Test set & Test statistics \\ \hline
		\multirow{3}{*}{MAC}  & CLEVR                       & 31h                                     & acc: 96.7\%                  & CLEVR    & 96.2\%          \\ \cline{2-6} 
		& \multirow{2}{*}{CoGenT - A} & \multirow{2}{*}{31h}                    & \multirow{2}{*}{acc: 97.0\%} & CoGenT-A & 96.9\%          \\ \cline{5-6} 
		&                             &                                         &                              & CoGenT-B & 79.5\%          \\ \hline
		\multirow{3}{*}{SMAC} & CLEVR                       & 28h                                     & acc: 95.8.\%                 & CLEVR    & 95.3\%          \\ \cline{2-6} 
		& \multirow{2}{*}{CoGenT-A}   & \multirow{2}{*}{28h}                    & \multirow{2}{*}{acc: 96.1\%} & CoGenT-A & 95.9\%          \\ \cline{5-6} 
		&                             &                                         &                              & CogenT-B & 78.7\%          \\ \hline
	\end{tabular}
	\caption{CLEVR \& CLEVR-CoGenT accuracy for the MAC \& simplified MAC model.}
	\label{results}
\end{table}

We report the obtained scores in \tableref{results}. Taking the evaluation of MAC on CLEVR as a reference experiment, it yields an accuracy of 96.2\%, obtained after 31h of training.\vmf{Not sure if we should explain in more details why there's a 3\% gap..}
Evaluating MAC on CoGenT shows that, like previous work (\cite{johnson2017inferring}, \cite{mascharka2018transparency})], the performance is worse on Condition B than Condition A after training only using Condition A data. As \tableref{results} shows, the MAC model achieves 79.5\% accuracy on Condition B, which surpasses the previously 78.8\% reported in \cite{perez2017film}. 
The simplified MAC reaches an accuracy of 95.3\% on CLEVR, which is close to the reference score. Moreover, the simplified MAC presents the advantage to train faster, showing a decrease of ~10\% in wall time. This point also holds true when training on CoGenT: not only does the simplified model trains faster, it also presents comparable performance when evaluated on both conditions.


Following \cite{johnson2017inferring}, we then fine-tune both models using 3k images and 30k questions from the Condition B data.. \textit{To continue later on}.