\subsection{Simplified MAC network}
Our proposed modification to the MAC network is based on two heuristic
simplifications of the MAC cell. 
First, we observe that when looking at the equations of different units
together as a whole, there are several linear layers
that are applied in sequence (with no activation in-between).
Therefore they can actually be combined to form a single linear layer.
The second observation uses the following heuristic assumption:
suppose there is an affine transformation that preserves the number
of dimensions. Then one can assume that the transformation is invertible
so as to avoid a loss in the information. Applying this principle to 
equations in the MAC cell results in eliminating one of the linear layers applied to
the knowledge base in the cell. With a careful reorganization, we in fact
can compute this operation on the knowledge base right after we apply the 2 
 CNN layers on the feature maps (i.e. prior to the reasoning steps). Thus, we can work purely
with this \emph{projected} knowledge base, that yields
noticeable savings in training time, as our experiments demonstrate
below, without incurring too much drop in accuracy.

In the description below, the original MAC cell equations are shown on the \emph{left}
while our simplified equations are shown (in color) on the {\color{Plum} \emph{right}}.
The equation numbering is the same as in~\cite{hudson2018compositional}.

\noindent\textit{Notation.}
Let $q$ denote the global context of the question
and let $\cw_s$ denote the contextual embedding of a word $s$ in the question.
For each tensor of dimension $H \times W \times d$ corresponding to an image,
its knowledge base $\{ \kb_{hw}\}$ is the set of $d$-dimensional vectors indexed by 
$h \in \{1,2,\dots,H\}$ and $w \in \{1,2,\dots, W\}$.
The index/position $i$ below denotes the reasoning step. 
Unless mentioned otherwise, the weights/biases are invariant w.r.t $i$, 
i.e., shared across all the reasoning steps. 

\noindent\textbf{Control unit:} 
For both models, in the control unit, the question $q$ is first transformed in each step of 
the reasoning using a \emph{position-aware}
linear layer depending on $i$: $q_i = U_i^{[d \times 2d]} q + b_i^{[d]}$.

\begin{multicols}{2}
	\noindent
	\begin{align*}
	&cq_i = W_{cq}^{[d \times 2d]} [c_{i-1}, q_i] + b_{cq}^{[d]}  \tag{c1} \\
	&ca_{is} = W_{ca}^{[1 \times d]} (cq_i \odot \cw_s) + b_{ca}^{[1]}
	\tag{c2.1}\\
	&cv_{is} = \textrm{softmax}(ca_{is}) \tag{c2.2}\\
	&\cc_i = \sum_s cv_{is} \, \cw_s  \tag{c2.3}
	\end{align*}
	\columnbreak
	{\color{Plum}
	\begin{align*}
	&cq_i = W_{cq}^{[d \times d]} c_{i-1} + q_i  \tag{c1} \\
	&ca_{is} = W_{ca}^{[1 \times d]} (cq_i \odot \cw_s)  \tag{c2.1}\\
	&cv_{is} = \textrm{softmax}(ca_{is}) \tag{c2.2}\\
	&\cc_i = \sum_s cv_{is} \, \cw_s  \tag{c2.3}
    \end{align*}}
\end{multicols}

\noindent\textbf{Read and write units:}
\begin{multicols}{2}
	\noindent
	\begin{align*}
	&I_{ihw} = (W_{m}^{[d \times d]} \mem_{i-1} + b_{m}^{[d]}) \\
	           & \qquad \quad \odot (W_{k}^{[d \times d]} \kb_{hw} + b_{k}^{[d]}) \tag{r1} \\
	&I'_{ihw} =  W_{I'}^{[d \times 2d]} [I_{ihw},\kb_{hw}]  + b_{I'}^{[d]}  \tag{r2} \\
	&ra_{ihw} = W_{ra}^{[1 \times d]} (\cc_i \odot I'_{ihw}) + b_{ra}^{[1]} \tag{r3.1}\\
	&rv_{ihw} = \textrm{softmax}(ra_{ihw}) \tag{r3.2}\\
	&\rr_i = \sum_s rv_{ihw} \, \kb_{hw}  \tag{r3.3}\\
	&\mem_i = W_{rm}^{[d \times d]} [\rr_i, \mem_{i-1}]  + b_{rm}^{[d]} \tag{w1}	
	\end{align*}
	\columnbreak
	{\color{Plum}
	\begin{align*}
	&I_{ihw} = \mem_{i-1} \odot \kb_{hw} \tag{r1} \\ \\
	&I'_{ihw} = W_{I'}^{[d \times d]} I_{ihw} + b_{I'}^{[d]} + \kb_{hw} \tag{r2} \\
	&ra_{ihw} = W_{ra}^{[1 \times d]} (\cc_i \odot I'_{ihw})  \tag{r3.1}\\
	&rv_{ihw} = \textrm{softmax}(ra_{ihw}) \tag{r3.2}\\
	&\rr_i = \sum_s rv_{ihw} \, \kb_{hw}  \tag{r3.3}\\
	&\mem_i = W_{rm}^{[d \times d]} \rr_i + b_{rm}^{[d]} \tag{w1}
	\end{align*}}
\end{multicols}
For the original MAC model shown on the left above, eqn.~(r1) can be slightly optimized for time
by computing the linear layer on the knowledge base \emph{prior} to the reasoning steps.
This induces additional space due to storing this projected knowledge base throughout the
reasoning steps.

\begin{table}
	\centering
	\begin{tabular}{ccccCcCc}
		\toprule
	     Model        & Read Unit               & Write Unit &  Control Unit         \\
		\midrule
		MAC   &  787,969 &  524,800        &    6,822,913    \\
		%\midrule
	    simplified MAC & 263,168  & 262,656       &    6,560,768 \\
	    \midrule
	     Reduction by [\%]  & 67\%  &   50\%       &      4\%  \\
		\bottomrule
	\end{tabular}
	\caption{Numbers of trainable parameters in the units of MAC and simplified MAC cells}
	\label{tab:parameters}
\end{table}

Our modifications resulted in a significant decrease in the number of parameters in the Read and Write units as well as a small drop in the Control unit, as presented in \tableref{tab:parameters}. Those three units combined together compose the MAC cell which is the core of the MAC network. It is important to emphasize that the MAC cell is the recurrent part of the network as opposed to the Input and Output units that are called only once. That is why we decided to focus on simplifying of those equations, whereas left the other two units unchanged.