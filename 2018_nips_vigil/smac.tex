\subsection{Simplified MAC network}
Our proposed modification to the MAC network is based on two heuristic
simplifications of the equations governing a MAC cell. 
First, we observed that when looking at the equations of different units
together as a whole, we notice that there are several affine transformations
that are applied in sequence (with no activation in-between).
Therefore they can actually be combined to form a single affine transformation.
The second observation uses the following heuristic argument:
suppose there is a single affine transformation which preserves the number
of dimensions. Then one can assume that the transformation is invertible
so as to avoid a loss in the information. Applying this principle to 
equations in the read and write unit results in eliminating an affine
operation of the knowledge base in the recurrent network. Instead we compute
this operation on the knowledge base right after we apply the 2 
trainable CNN layers (i.e. prior to the reasoning steps). This results
in a significant savings in training time, as our experiments demonstrate
below, without incurring too much drop in accuracy.

In the description below, the original MAC cell equations are shown on the \emph{left}
while our simplified equations are shown (in color) on the {\color{Plum} \emph{right}}.
(The equation numbering is the same as in~\cite{hudsonManning18}.)

\noindent\textit{Notation:} The index/position $i$ below denotes the reasoning step. 
Unless mentioned otherwise, the weights/biases are invariant w.r.t $i$, 
i.e., shared across all the reasoning steps. 

\noindent\textbf{Control unit:} 
For both models, in the control unit, the question $q$ is first transformed in each step of 
the reasoning using a \emph{position-aware}~\cite{hudsonManning18} 
linear layer: $q_i = U_i^{[d \times 2d]} q + b_i^{[d]}$.

\begin{multicols}{2}
	\noindent
	\begin{align*}
	&cq_i = W_{cq}^{[d \times 2d]} [c_{i-1}, q_i] + b_{cq}^{[d]}  \tag{c1} \\
	&ca_{is} = W_{ca}^{[1 \times d]} (cq_i \odot \cw_s) + b_{ca}^{[1]}
	\tag{c2.1}\\
	&cv_{is} = \textrm{softmax}(ca_{is}) \tag{c2.2}\\
	&\cc_i = \sum_s cv_{is} \, \cw_s  \tag{c2.3}
	\end{align*}
	\columnbreak
	{\color{Plum}
	\begin{align*}
	&cq_i = W_{cq}^{[d \times d]} c_{i-1} + q_i  \tag{c1} \\
	&ca_{is} = W_{ca}^{[1 \times d]} (cq_i \odot \cw_s)  \tag{c2.1}\\
	&cv_{is} = \textrm{softmax}(ca_{is}) \tag{c2.2}\\
	&\cc_i = \sum_s cv_{is} \, \cw_s  \tag{c2.3}
    \end{align*}}
\end{multicols}

\noindent\textbf{Read and write units:}
For both models below, $\kb_{hw}$ denotes the \emph{knowledge base}~\cite{hudsonManning18}
consisting of 3D tensors with dimension $H \times W \times d$ for each image
in the data set. The difference in computing $\kb_{hw}$ is that the simplified MAC uses
an additional linear layer on top of the 2 CNN layers present in the original MAC network.
This circumvents the need for performing additional linear transformations on the 
knowledge base, as is done by the original MAC network in \emph{each} reasoning step.

\begin{multicols}{2}
	\noindent
	\begin{align*}
	&I_{ihw} = (W_{m}^{[d \times d]} m_{i-1} + b_{m}^{[d]}) \\
	           & \qquad \quad \odot (W_{k}^{[d \times d]} \kb_{hw} + b_{m}^{[d]}) \tag{r1} \\
	&I'_{ihw} =  W_{I'}^{[d \times 2d]} [I_{ihw},\kb_{hw}]  + b_{I'}^{[d]}  \tag{r2} \\
	&ra_{ihw} = W_{ra}^{[1 \times d]} (\cc_i \odot I'_{ihw}) + b_{ra}^{[1]} \tag{r3.1}\\
	&rv_{ihw} = \textrm{softmax}(ra_{ihw}) \tag{r3.2}\\
	&\rr_i = \sum_s rv_{ihw} \, \kb_{hw}  \tag{r3.3}\\
	&\mem_i = W_{rm}^{[d \times d]} [\rr_i, \mem_{i-1}]  + b_{rm}^{[d]} \tag{w1}	
	\end{align*}
	\columnbreak
	{\color{Plum}
	\begin{align*}
	&I_{ihw} = \mem_{i-1} \odot \kb_{hw} \tag{r1} \\ \\
	&I'_{ihw} = W_{I'}^{[d \times d]} I_{ihw} + b_{I'}^{[d]} + \kb_{hw} \tag{r2} \\
	&ra_{ihw} = W_{ra}^{[1 \times d]} (\cc_i \odot I'_{ihw})  \tag{r3.1}\\
	&rv_{ihw} = \textrm{softmax}(ra_{ihw}) \tag{r3.2}\\
	&\rr_i = \sum_s rv_{ihw} \, \kb_{hw}  \tag{r3.3}\\
	&\mem_i = W_{rm}^{[d \times d]} \rr_i + b_{rm}^{[d]} \tag{w1}
	\end{align*}}
\end{multicols}

