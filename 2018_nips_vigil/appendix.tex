\appendix

\section{Description of datasets}

Most of the VQA datasets contain ambiguities and have strong linguistic biases that allow a model to learn answering strategies that exploit those biases, without reasoning about the visual input ~\cite{Santoro2017ASN}.
The CLEVR dataset ~\cite{johnson2017clevr}  \href{url}{https://arxiv.org/pdf/1612.06890.pdf}  was developed to address those issues and come back to the core challenge of visual QA which is reasoning  .\\
CLEVR contains images of 3D-rendered objects, such as spheres and cylinders. Each image is associated with a number of questions that fall into different categories. For example, query attribute questions may ask “What is the color of the sphere?”, while compare attribute questions may ask “Is the cube the same material as the cylinder? ”.
The dataset consist of:
\begin{itemize}
\item 	A training set of 70k images and 700k questions
\item	A test  set of 15k images and 150k questions 
\item	A validation set of 15k images and 150k  questions
\item	Answers, scene graphs and functional programs for all train and val images and questions

\end{itemize}

The dataset features unbiased, highly compositional questions that require an array of challenging reasoning skills, such as transitive and logical relations, counting and comparisons, without allowing any shortcuts around such reasoning ~\cite{hudson2018compositional}.

To test how well models learn compositional concepts that generalize, CLEVR-CoGenT was introduced . This dataset is synthesized in the same way as CLEVR but contains two conditions. As shown in Table 3 in Condition A, all cubes are gray, blue, brown, or yellow and all cylinders are red, green, purple, or cyan; in Condition B, cubes and cylinders swap color palettes.
Both conditions contain spheres of all colors. CLEVR-CoGenT thus indicates how a model answers CLEVR questions: by memorizing combinations of traits or by learning disentangled or general representations ~\cite{perez2017film}

This dataset consists of:
\begin{itemize}
\item	A training set of 70,000 images and 699,960 questions in Condition A
\item	A validation set of 15,000 images and 150,000 questions in Condition A
\item	A validation set of 15,000 images and 149,991 questions in Condition B
\item	A test set of 15,000 images and 149,980 questions in Condition A
\item	A test set of 15,000 images and 149,992 questions in Condition B
\item	Answers, scene graphs and functional programs for all train and val images and questions
\end{itemize}

\begin{table}
	\centering
	\begin{tabular}{ccccCcCc}
		\toprule
		Dataset        & Cubes              & Cylinders &  Spheres         \\
		\midrule
		CLEVR   &  any color &  any color        &    any color    \\
		%\midrule
		CLEVR CoGenT A & gray / blue / brown / yellow  & red / green / purple / cyan       &    any color  \\
		CLEVR CoGenT B  & red / green / purple / cyan &   gray / blue / brown / yellow       &      any color  \\
		\bottomrule
	\end{tabular}
	\caption{Comparison of the different colors/shapes combinations between CLEVR, CLEVR CoGenT-A and CLEVR CoGenT-B  }
	\label{tab:parameters}
\end{table}

\section{Full MAC and S-MAC comparison}

We think that in the main paper we should leave:
\begin{itemize}
\item first row - MAC - train on CLEVR, test on CLEVR 
\item all rows regaring S-MAC
\end{itemize}

When we will fill the whole table we will copy the results in here.

\section{Illustration of failures of MAC on CLEVR}

\begin{itemize}
\item one with color failure -- that the attention is pointing at the right object, whereas the answer is improper because he never saw this combination
\item one with shape failure -- similarly	
\end{itemize}
Pictures with some description