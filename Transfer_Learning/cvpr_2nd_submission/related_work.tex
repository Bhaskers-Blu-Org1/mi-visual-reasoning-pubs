\section{Related work}
\label{sec:related_work}

In Computer Vision, it is now standard practice to pretrain an image encoder (such as VGG~\cite{simonyan2014very} or ResNet~\cite{he2016deep}) on large-scale datasets (such as ImageNet~\cite{deng2009imagenet}), and reuse the weights in unrelated domains and tasks, such as segmentation of cars~\cite{iglovikov2018ternausnet} or Visual Question Answering (VQA) in a medical domain~\cite{kornuta2019leveraging}.
Such performance improvements are appealing, especially in cases where both the domain (natural vs. medical images) and the task (image classification vs. image segmentation vs. VQA) change significantly.

Similar developments have emerged in the Natural Language Processing (NLP) community.
Using shallow word embeddings, such as word2vec~\cite{mikolov2013distributed} or GloVe~\cite{pennington2014glove}, pretrained on large corpuses from e.g.\ Wikipedia or Twitter, has been a standard procedure across different NLP domains and tasks.
Recently, there is a clear, growing trend in the utilization of deep contextualized word representations such as ELMo~\cite{peters2018deep} (based on bidirectional LSTMs~\cite{hochreiter1997long}) or BERT~\cite{devlin2018bert} (based on the Transformer~\cite{vaswani2017attention} architecture), where entire deep networks (not just the input layer) are pretrained on very large corporas.
%In analogy to repositories with pretrained image encoders in TensorFlow~\cite{} or PyTorch~\cite{}, the NLP community has also started to create model repositories, some with dozens of pretrained models ready to be downloaded and used. HuggingFace~\cite{wolf2019transformers} is one of the most notable examples.

Visual Reasoning tasks, combining both the visual and language modalities~\cite{mogadala2019trends}, naturally draw from those findings by reusing the pretrained image and word/question encoders.
Ensuring models can generalize on high-level, abstract \emph{reasoning} has been a recent interest of the community.
The first attempt at establishing a framework enabling transfer learning of reasoning skills was CLEVR~\cite{johnson2017clevr} with its two CoGenT variants.
This fostered research~\cite{mascharka2018transparency,perez2018film,johnson2017inferring,marois2018transfer}, proposing models designed to transfer from one domain to another, with different distributions of visual attribute combinations.
Similarly, the baseline model introduced alongside the COG dataset~\cite{yang2018dataset} has shown the ability to solve tasks not explicitly trained on, by leveraging knowledge learned on other tasks.

These results clearly indicate the usefulness of transfer learning, despite the assumption of similar distributions between the source and target domains not being respected.
Conjointly, transfer learning raises several research questions, such as the characteristics which make a whole dataset or a particular task more favorable to be used in pretraining (notably ImageNet~\cite{huh2016makes}), or regarding the observed performance correlation of models with different architectures between the source and target domains~\cite{kornblith2019better}.
One of the most systematic works in this area is the computational taxonomic map for task transfer learning~\cite{zamir2018taskonomy}, aiming at discovering the dependencies between twenty-six different computer vision tasks.
In this work, we extend this idea further and introduce a taxonomy allowing us to isolate and quantify different aspects of visual reasoning.
%We illustrate it with the two above mentioned CLEVR and COG datasets.
%The taxonomy enabled us to perform more systematic studies and facilitate comparison between the already existing solutions and the newly introduced SAMNet model.


%transfer learning has helped models generalize better to domains with different distributions.


%We propose to address this by introducing a taxonomy of transfer learning in visual reasoning, and illustrate it with two visual reasoning datasets.


% \tk{The experimental results clearly indicate that transfer learning is helping, despite that the core assumptions on the compatibility/similarity of domains is broken (transfering between domains having similar distribution).
% We do not understand this fully, thus the need for a more systematic research on that topic emerges.
% The paper adresses that by introducing a theoretical framework/taxonomy enabling to categorise visual reasoning tasks.
% As a starting point, we picked two visual reasoning datasets and analyse the achieved results through the prism of the proposed taxonomy.
% }
%
%
% \tk{after reading that section the reader should end up with a conclusion that: there are no good models for TL in VR and, moreover, the datasets are randomly testing this or that, there is no theoretical framework showing that do they mean/bigger picture is missing}
