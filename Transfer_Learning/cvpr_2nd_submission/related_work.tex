\section{Related work}
\label{sec:related_work}

In Computer Vision, it is now standard practice to pretrain an image encoder (such as VGG~\cite{simonyan2014very} or ResNet~\cite{he2016deep}) on large-scale datasets (such as ImageNet~\cite{deng2009imagenet}), and reuse the weights in unrelated domains and tasks, such as segmentation of cars~\cite{iglovikov2018ternausnet} or Visual Question Answering (VQA) in a medical domain~\cite{kornuta2019leveraging}.
Such performance improvements are appealing, especially in cases where both the domain (natural vs. medical images) and the task (image classification vs. image segmentation vs VQA) change significantly.

Similar developments have emerged in the Natural Language Processing (NLP) community.
Using shallow word embeddings, such as word2vec~\cite{mikolov2013distributed} or GloVe~\cite{pennington2014glove}, pretrained on large corpuses from e.g.\ Wikipedia or Twitter, was de facto a standard procedure accross different NLP domains and tasks for the last several years.
Recently, there is a clear, growing trend of utilization of deep contextualized word representations such as ELMo~\cite{peters2018deep} (based on bidirectional LSTMs~\cite{hochreiter1997long}) or BERT~\cite{devlin2018bert} (based on the Transformer~\cite{vaswani2017attention} architecture), where entire deep networks (not just the input layer) are pretrained on very large corporas.
%In analogy to repositories with pretrained image encoders in TensorFlow~\cite{} or PyTorch~\cite{}, the NLP community has also started to create model repositories, some with dozens of pretrained models ready to be downloaded and used. HuggingFace~\cite{wolf2019transformers} is one of the most notable examples.

Visual Reasoning tasks combining both the visual and language modalities~\cite{mogadala2019trends} naturally draw from those findings by reusing the pretrained image and word/question encoders.
The community also seems to be realising that the models should not only be able to generalize on the low-level image features, but also on the performed high-level, abstract \textit{reasoning}.
First notable attemp to establish a framework enabling transfer learning of reasoning skill was CLEVR~\cite{johnson2017clevr} with its two CoGenT variants.
As a result several interesting works appeared~\cite{mascharka2018transparency, perez2018film, johnson2017inferring,marois2018transfer}, proposing models trying to cope with transfer from one to the other domain with different distributions of visual attribute combinations.
Similarly, the baseline model introduced along with the COG dataset~\cite{yang2018dataset} have shown that it can do significantly better answer questions belonging to task classes not being explicitly trained on when leveraging knowledge learned on other tasks.
These results clearly indicate the usefulness of transfer learning, despite the assumption of similar distributions between the source and target domains not being respected.
At the same time transfer learning several research questions, such as the characteristics which make a whole dataset or a particular task more favorable to be used in pretraining (notably ImageNet~\cite{huh2016makes}), or regarding the observed performance correlation of models with different architectures between the source and target domains~\cite{kornblith2019better}.
One of the most systematic works in this area is the computational taxonomic map for task transfer learning~\cite{zamir2018taskonomy}, aiming at discovering the dependencies between twenty-six different computer vision tasks.
In this work we extend that idea further and introduce a taxonomy enabling us to isolate and quantify different aspects of visual reasoning on the performance of our model.
%We illustrate it with the two above mentioned CLEVR and COG datasets.
%The taxonomy enabled us to perform more systematic studies and facilitate comparison between the already existing solutions and the newly introduced SAMNet model.


%transfer learning has helped models generalize better to domains with different distributions. 


%We propose to address this by introducing a taxonomy of transfer learning in visual reasoning, and illustrate it with two visual reasoning datasets.


% \tk{The experimental results clearly indicate that transfer learning is helping, despite that the core assumptions on the compatibility/similarity of domains is broken (transfering between domains having similar distribution).
% We do not understand this fully, thus the need for a more systematic research on that topic emerges.
% The paper adresses that by introducing a theoretical framework/taxonomy enabling to categorise visual reasoning tasks.
% As a starting point, we picked two visual reasoning datasets and analyse the achieved results through the prism of the proposed taxonomy.
% }
%
%
% \tk{after reading that section the reader should end up with a conclusion that: there are no good models for TL in VR and, moreover, the datasets are randomly testing this or that, there is no theoretical framework showing that do they mean/bigger picture is missing}
