\section{Introduction}
%dummy
In recent years, neural networks, being at the epicenter of the Deep Learning~\cite{lecun2015deep} revolution, became the dominant solutions across many domains, from Speech Recognition~\cite{graves2013speech}, Image Classification~\cite{krizhevsky2012imagenet}, Object Detection~\cite{redmon2016you}, to Question Answering~\cite{weston2014memory} and Machine Translation~\cite{bahdanau2014neural} among others.
Neural networks, being statistical models~\cite{ripley1993statistical,warner1996understanding}, rely on the assumption that training and testing samples are independent and identically distributed (\textit{iid}); i.e.\ sampled from a common input space under similar data distribution characteristics.
However, in many real-world scenarios, this assumption does not hold. Moreover, as modern neural models often have millions of trainable parameters, training them requires vast amounts of data, which for some domains (e.g., medical) can be very expensive and/or extremely difficult to collect.
One of the widely used solutions is Transfer Learning~\cite{pan2009survey,weiss2016survey}, a technique which enhances model performance by transferring \emph{information} from one domain to another.


In this work we focus on transfer learning in multi-modal reasoning tasks combining vision and language~\cite{mogadala2019trends}.  Recently introduced visual reasoning datasets, such as ~\cite{johnson2017clevr,yang2018dataset,song2018explore}, attempt to probe the generalization capabilities of models under various transfer learning scenarios. For example, CLEVR-CoGenT offers two dataset variants with different combinations of visual attributes. Whereas, the COG dataset contains two variants with different number of frames and scene complexity.  Both datasets come with a number of task classes that require similar reasoning abilities (e.g. compare or count objects). 
The open question is whether we can build a neural architecture that fosters transfer learning and generalizes from simpler to more complex reasoning tasks.


\noindent The contributions of this paper are the following:
%Consequently:
\begin{enumerate}
	%\compresslist
	\item We propose a new model, called SAMNet (Selective Attention Memory Network), which achieves state-of-the-art results on COG~\cite{yang2018dataset}, a Video QA reasoning dataset.
	\item We propose a taxonomy of transfer learning, inspired from~\cite{pan2009survey}, applied to the domain of visual reasoning. Articulated around 3 main axes, we illustrate it through the COG dataset, as well as the CLEVR~\cite{johnson2017clevr} diagnostic dataset for Image QA.
	\item Subsequently, we measure the impact of transferring the whole pretrained SAMNet model in the 3 proposed transfer learning settings: feature transfer, temporal transfer and reasoning transfer. This analysis is supported by an extensive set of experiments using the COG and CLEVR datasets, as well as their variants. Several of these experiments show significant transfer learning capabilities of SAMNet.
\end{enumerate}