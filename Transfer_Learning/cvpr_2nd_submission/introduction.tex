\section{Introduction}
%dummy
In recent years, neural networks, being at the epicenter of the Deep Learning~\cite{lecun2015deep} revolution, became the dominant solutions across many domains, from Speech Recognition~\cite{graves2013speech}, Image Classification~\cite{krizhevsky2012imagenet}, Object Detection~\cite{redmon2016you}, to Question Answering~\cite{weston2014memory} and Machine Translation~\cite{bahdanau2014neural} among others.
Neural networks, being statistical models~\cite{ripley1993statistical,warner1996understanding}, rely on the assumption that training and testing samples are independent and identically distributed (\textit{iid}); i.e.\ sampled from a common input space under similar data distribution characteristics.
However, in many real-world scenarios, this assumption does not hold. Moreover, as modern neural models often have millions of trainable parameters, training them requires vast amounts of data, which for some domains (e.g., medical) can be very expensive and/or extremely difficult to collect.
One of the widely used solutions is Transfer Learning~\cite{pan2009survey,weiss2016survey}, a technique which enhances model performance by transferring \emph{information} from one domain to another.


In this work we focus on transfer learning in multi-modal reasoning tasks combining vision and language~\cite{mogadala2019trends}.  Recently introduced visual reasoning datasets, such as ~\cite{johnson2017clevr,yang2018dataset,song2018explore}, attempt to probe the generalization capabilities of models under various transfer learning scenarios. For example, CLEVR/CoGenT \cite{johnson2017clevr} offers two dataset variants with different combinations of visual attributes. Whereas, the COG \cite{yang2018dataset}, a Video QA reasoning dataset, contains two variants with different number of frames and scene complexity.  Both datasets come with a number of task classes that require similar reasoning abilities (e.g. compare or count objects). 
The open question is whether we can build a neural architecture that fosters transfer learning and generalizes from simpler to more complex reasoning tasks.


\noindent The contributions of this paper are the following:
%Consequently:
\begin{enumerate}
	%\compresslist
	\item We introduce a new model called SAMNet (Selective Attention Memory Network).
	The unique features of SAMNet include the dynamic processing of video input frame-by-frame, multi-step reasoning over a given image and remembering the relevant concepts.
	
	\item We propose a taxonomy of transfer learning to highlight new research directions, identify missing areas and encourage new datasets for transfer learning in visual reasoning. We evaluated the CLEVR/CoGenT and COG tasks to show how they map onto the proposed theoretical framework.
	\item We performed an extensive set of experiments, comparing our model with baselines on the existing tasks, achieving the state-of-the-art results on COG~\cite{yang2018dataset}. We also designed two new tasks for reasoning transfer and show that the design of SAMNet  promotes transfer learning capabilities and generalization.
	
\end{enumerate}