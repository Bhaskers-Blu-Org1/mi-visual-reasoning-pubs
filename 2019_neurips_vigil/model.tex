\section{Selective Attention Memory (SAM) Network}

%\begin{figure}[!b]
%\begin{minipage}{0.43\textwidth}
%	\centering
%	\includegraphics[width=\textwidth]{../img/architecture/samnet_architecture4}
%\end{minipage}\hfill
%\begin{minipage}{0.55\textwidth}
%	\centering
%	\includegraphics[width=\textwidth]{../img/architecture/samcell_reasoning}
%\end{minipage}\hfill
%\caption{General architecture of SAMNet (left) and a single reasoning step in SAMCell (right)}
%\label{fig:samnet}
%\end{figure}	

%Selective Attention Memory Network (SAMNet) is a end-to-end differentiable model made for video reasoning. It is a model based on attention mechanisms but also on a Selective Attention Memory which is able to store selected entities. This memory enables SAMNet to reason across multiple frames and perform spatio-temporal reasoning. 
%The core of SAMNet is based a recurrent cell called SAMCell. By aligning together a series of k SAMCells per frame, the network can perform k reasoning steps over a frame. At every new frame, a new series of k SAMCells is initiated. The SAMCell can read and write to memory at every frame using a content addressable mechanism. This section describes the model and the different units that composed a SAMCell. They are called the Question-driven Controller, the Visual Retrieval Unit, the Memory Retrieval Unit, the Reasoning unit, the Memory update unit, and the Summary Object Udpate Unit. 
%The model is also composed of an Image Encoder and a Question Encoder both responsible to pre-process the visual and textual inputs. The output unit is a classifier.
%All those modules are described below.

\begin{figure}[hbtp]
	\centering
	\includegraphics[width=\textwidth]{../img/architecture/samnet_architecture4}
	\caption{General architecture of SAMNet}
	\label{fig:samnet}
\end{figure}	

SAM Network (SAMNet for short) is an end-to-end differentiable recurrent model equipped with an external memory for enabling multi-step reasoning over text and video (\cref{fig:samnet}).
The model makes a single pass over the frames in temporal order, accessing one frame at a time.
The memory locations store relevant objects representing contextual information about words in text and visual objects extracted from video. 
Each location of the memory stores a $d$-dimensional vector. %, where $d$ is a global parameter.
The memory  can be accessed through either content-based addressing, via dot-product attention, or location-based addressing. 
Coupled with gating mechanisms to be described later, this enables correct objects to be retrieved 
in order to perform spatio-temporal reasoning on frames and text. 
%A notable feature of this design is that the number of addresses $N$ can be set to different values during training and 
%testing to fit the characteristics of data.

\begin{figure}[hbtp]
	\centering
	\includegraphics[width=\textwidth]{../img/architecture/samcell_reasoning}
	\caption{Single reasoning step in SAMCell}
	\label{fig:samcell}
\end{figure}	

The core of SAMNet is a recurrent cell called a SAM Cell (\cref{fig:samcell}). 
By aligning together a new series of $k$ SAM Cells per frame, the network can perform $k$ 
reasoning steps over each frame, with information flowing between frames through the external memory. 
While processing a single frame, for $t=1,2, \dots, k$, SAMCell maintains the following information as part of its recurrent state:
(a) $c_t \in \Reals^d$, the control state used to drive the reasoning over objects in the frame and memory; and
(b) $so_t  \in \Reals^d$, the summary visual object representing the relevant object for step $t$.
Let $M_t \in  \Reals^{N \times d}$ denote the external memory with $N$ slots at the end of step $t$.
Let $\whead_t \in  \Reals^N$ denote an attention vector over the memory locations;
in a trained model, $\whead_t$ points to the location of first empty slot in memory for adding new objects.   

\paragraph{Question-driven Controller.}

%The Question-driven Controller plays an important role in the reasoning process.
%It drives the attention over the question and produces the new control states. Each new control state defines a new reasoning operation. The inputs of this unit are the past control state, the question encoding and the contextual words (see Question Encoding Unit). It uses the dot product attention between the contextual words
%and the combination of the past control states and the question encoding.  This attention layer produces the new control state.
%
%This unit also outputs the temporal class weights that will be used in the Reasoning Unit. It gives access to a temporal information for the current words (last, latest, now, none temporal).

This module drives attention over the question to produce $k$ control states, one per reasoning operation. 
The control state $c_t$ at step $t$ is then fed to temporal classifier, a two-layer feedforward network with ELU activation in the hidden layer of $d$ units.
The output $\tau_t$ of the classifier is intended to represent the different temporal contexts (or lack thereof) associated with the word in focus for that step of reasoning.	
For example in case of the COG dataset we pick 4 classes to capture the terms labeled ``last'', ``latest'', ``now'', and ``none''.

\paragraph{Reasoning Unit}
This module is the backbone of SamNet that determines what gating operations need to be performed on the external memory as well as determining whether the relevant object for reasoning is supposed to be retrieved from the current frame or from the external memory.




This section describes the model and the different units that composed a SAMCell. They are called the Question-driven Controller, the Visual Retrieval Unit, the Memory Retrieval Unit, the Reasoning unit, the Memory update unit, and the Summary Object Udpate Unit. 
The model is also composed of an Image Encoder and a Question Encoder both responsible to pre-process the visual and textual inputs. The output unit is a classifier.
All those modules are described below.



\subsection{Visual Retrieval Unit}

The visual retrievial unit is responsible to extract visual information from the current image given a control state coming from the Question-driven Controller. It is first projecting the past summary object and the feature maps together using the interaction module.
It is then using the attention module as follow. The query are the control states and the keys and are the feature maps coming from the image encoder. The results of this attention is applied on the modified features maps coming from the interaction module.  
This unit outputs the extracted object and the visual attention.

\subsection{Memory Retrieval Unit}

The role of the memory retrieval unit is to read and extract object from memory).
As the Visual Retrieval Unit, it uses the combination of the two following submodules. The interaction module blends together the extracted object and the content of the memory. The attention module then extract the corresponding object in memory if present. This unit outputs the extracted object and its corresponding location, we call it the "read head".


\subsection{Reasoning Unit}


\subsection{Memory update Unit}

This unit is meant to update the content of the memory. 

Three actions can happen:

\begin{itemize}
	\item There is no object to be added to memory, the memory remains unchanged
	\item There is one object that needs to be added to memory, but a similar object is already in memory at a given location. The new object will replace the old object at this location
	
	\item There is one object that needs to be added to memory, and  it is a new object. It is added at the write head location.
\end{itemize}


This module also updates the position of the write head. If a new object as been added to the current write head position, the right head shifts right to a new empty slot. If the object has been replaced, the write head doesn't move.

\subsection{Summary  Unit}

The  Summary Unit is the last unit of the SAMCell. It is responsible to output the new summary object. It first picks which object is relevant between the object extracted from memory and the visual object extracted from the image. Once the relevant object is picked, it is combined with the former summary object through a linear layer to become the new summary object. It is the final step of the SAMCell reasoning cycle. 

The image encoder, question encoder and output unit are described in the appendix.
