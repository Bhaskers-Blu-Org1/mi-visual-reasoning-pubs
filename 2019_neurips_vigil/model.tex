\section{Model}

Selective Attention Memory Network (SAMNet) is a end-to-end differentiable model made for video reasoning. It is a model based on attention mechanisms but also on a Selective Attention Memory which is able to store selected entities. This memory enables SAMNet to reason across multiple frames and perform spatio-temporal reasoning. 
The core of SAMNet is based a recurrent cell called SAMCell. By aligning together a series of k SAMCells per frame, the network can perform k reasoning steps over a frame. At every new frame, a new series of k SAMCells is initiated. The SAMCell can read and write to memory at every frame using a content addressable mechanism. This section describes the model and the different units that composed a SAMCell. They are called the Question-driven Controller, the Visual Retrieval Unit, the Memory Retrieval Unit, the Reasoning unit, the Memory update unit, and the Summary Object Udpate Unit. 
The model is also composed of an Image Encoder and a Question Encoder both responsible to pre-process the visual and textual inputs. The output unit is a classifier.
All those modules are described below.


\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{../img/architecture/samnet_architecture}
	\caption{}
	\label{fig:samnetarchitecture}
\end{figure}
	
	

\subsection{Question-driven Controller}

The Question-driven Controller plays an important role in the reasoning process.
It drives the attention over the question and produces the new control states. Each new control state defines a new reasoning operation. The inputs of this unit are the past control state, the question encoding and the contextual words (see Question Encoding Unit). It uses the dot product attention between the contextual words
and the combination of the past control states and the question encoding.  This attention layer produces the new control state.

This unit also output the temporal class weights that will be used in the Reasoning Unit. It gives access to a temporal information for the current words. (last, latest, now, none temporal)

\subsection{Visual Retrieval Unit}

The visual retrievial unit is responsible to extract visual information from the current image given a control state coming from the Question-driven Controller. It is first projecting the past summary object and the feature maps together using the interaction module.
It is then using the attention module as follow. The query are the control states and the keys and are the feature maps coming from the image encoder. The results of this attention is applied on the modified features maps coming from the interaction module.  
This unit outputs the extracted object and the visual attention.

\subsection{Memory Retrieval Unit}

The role of the memory retrieval unit is to read and extract object from memory (Selective Attention Memory).
As the Visual Retrieval Unit, it uses a combination of two following submodules. The interaction module responsible to blend the extracted object and the content of the memory. The attention module then extract the corresponding object in memory if present.


\subsection{Reasoning Unit}


\subsection{Memory update Unit}

This unit is meant to update the content of the memory. It is controlled by two booleans that decides if the object needs to be added to a new position, or replaced somewhere. 
It also updated the position of the write head. If a new object as been added to the current write head position, the right head shifts right to a new empty slot. If the object has been replaced, the write head doesn't move.

\subsection{Summary Object  Unit}

The goal of the Summary Object 

The image encoder, question encoder and output unit are described in the appendix.
