\section{Appendix}

\section{VWM model}

\begin{notation}
Treat 1D tensors as column vectors and 2D tensors as matrices, where appropriate.
We use lower case to represent both 1D and 2D tensors but occasionally use upper case
for 2D tensors where matrix operations are involved.

\begin{enumerate}
	\item Let 
	$\Delta^d = \{ (x_0, x_1, \dots, x_d) : x_0 + x_1 + \dots + x_d = 1, x_i \ge 0, i = 0, 1, \dots, d\}$ denote the standard $d$-simplex.	
	
	\item  Let $\circ$ denote concatenation of two tensors with identical shape except possibly
	for their last dimensions $d_1$ and $d_2$, respectively,  
	resulting in a tensor with last dimension of $d_1+d_2$. 
	
	\item Let $\odot$ denote  element-wise product of two tensors of same shape,
	i.e., Hadamard product for vectors/matrices.
	
	\item Let $\otimes$ denote tensor product of two tensors, 
	i.e. Kronecker product for vectors/matrices.
\end{enumerate}
\end{notation}	

\section{Basic layers/modules}

\colorbx{Linear (Affine) Layer}

\begin{description}
	\item[Inputs:] A tensor $x$ with last dimension $n$.
	\item[Parameter:] An affine function $\cG: \Reals^n \to \Reals^m$ with 
	weight and bias parameters.
		
	\item[Output:] A tensor $y$ with last dimension $m$, and remaining dimensions
	same as that of $x$, obtained by applying $\cG$ to each 1D slice of $x$
	along the last dimension.
	\end{description}


\colorbx{Attention Module}

\begin{description}
	\item[Inputs:] 
	\begin{enumerate}
		\item[]
		\item Query: $q \in \Reals^d$
		\item Keys: $K \in \Reals^{N \times d}$	
		\item Values: $V \in \Reals^{N \times d}$. By default $V=K$, unless mentioned explicitly.	
	\end{enumerate}

	\item[Parameter:] Weight $w \in \Reals^d$

	\item[Outputs:] 
	\begin{enumerate}
		\item[]
		\item Content vector: $h =  V^{\T} u \in \Reals^d$
		\item Attention vector:  $w = \softmax(K(w \odot q)) \in \Reals^N$
	\end{enumerate}
\end{description}


\colorbx{Interaction Module}

\begin{description}
	\item[Inputs:] 
	\begin{enumerate}
		\item[]
		\item Base object: $b \in \Reals^d$
		\item Feature objects: $f \in \Reals^{M \times d}$	
	\end{enumerate}
	
	\item[Parameters:] 
	\begin{enumerate}
	\item[]
	\item Base object projection linear layer: $\cG: \Reals^d \to \Reals^d$
	\item Feature objects projection linear layer : $\cK: \Reals^{M \times d} \to \Reals^{M \times d}$
	\item Modifier linear layer:  $\cH: \Reals^{M \times 2d} \to \Reals^{M \times d}$	
\end{enumerate}
		
	\item[Output:] Modified feature objects 
	$f' =  \cH( \cK(f) \odot ( \vone \otimes \cG(b))) \in  \Reals^{M \times d}$
\end{description}

\hrulefill

\section{VWM cell}

The VWM recurrent cell is executed for $T$ reasoning steps for every frame in
the temporal order.  Within a single frame, the cell state at the end of each reasoning step 
$t=1,2, \dots, T$ is denoted by $(c_t, M_t, o_t)$, where: 
\begin{enumerate}
	\item $c_t \in \Reals^d$ is the control state;
	\item $M_t \in  \Reals^{N \times d}$ is the visual working memory with $N$ slots;
	\item $w_t \in  \Reals^N$ is the write head; and
	\item $so_t  \in \Reals^d$ is the summary visual object.
\end{enumerate} 
The initial state is such that both $c_0$ and $so_0$ are initialized
to a fixed value at the start of each frame. However $M_0$ is initialized only once at
the start of the first frame and otherwise taken to be the value of $M_T$ at the
end of the previous frame.

%The number of slots $N$ for the VWM $M_t$ is not fixed because the neural network 
%parameters do not depend on it. It can be variable across the different datasets used
%for training, validation and test as well as within each dataset. This, for example, enables a 
%form of transfer learning where we can train on an easy dataset for one value of $N$
%and study its generalization to a hard dataset using a larger value of $N$.

\colorbx{Question-driven Controller}

\begin{description}
	\item[Inputs:] 
	\begin{enumerate}
		\item[]
		\item Reasoning step $t = 1,2, \dots, T$
		\item Previous control state: $c_{t-1} \in \Reals^d$	
		\item Contextual words: $cw \in \Reals^{L \times d}$
		\item Question encoding: $q \in \Reals^d$
	\end{enumerate}
	
	\item[Parameters:] 
	\begin{enumerate}
		\item[]
		\item Reasoning step-dependent linear layer: $\cG_t: \Reals^d \to \Reals^d$, depending on $s$
		\item Concatenation linear layer: $\cH: \Reals^{2d} \to \Reals^d$
		\item Attention module $\cA$
		\item Temporal classifier:  $\cK: \Reals^d \to \Delta^3$. A two-layer feedforward
		network with ELU activation in the hidden layer of $d$ units.	
		The classes for the temporal context are labeled ``last'', ``latest'', ``now'', as well as 
		a fourth class label ``none`` indicating no temporal context.
		If $\tau \in \Delta^3$ is the output of the classifier, we denote the components by
		$\tlast$, $\tlatest$, $\tnow$ and $\tnone$.
	\end{enumerate}
	
	\item[Outputs:] 
	\begin{enumerate}
		\item[]
		\item Control state $c_t \in \Reals^d$
		\item Control attention $ca_t \in \Reals^L$
		\item Temporal class weights $\tau_t \in \Reals^4$
    \end{enumerate}
	
	\item[Equations:] 
	\begin{enumerate}
		\item[]
		\item Modulation: $y = \cH\bigl([c_{t-1}, \cG_t(q)]\bigr)$
		\item Control state and attention: $c_t,  ca_t= \cA(y, cw)$
		\item Temporal classification: $\tau_t = \cK(c_t)$
\end{enumerate}
\end{description}


\colorbx{Visual Retrieval Unit}

\begin{description}
	\item[Inputs:] 
	\begin{enumerate}
		\item[]
		\item Control state: $c_t \in \Reals^d$	
		\item Previous summary object: $so_{t-1} \in \Reals^d$
		\item Feature map of current frame: $F \in \Reals^{H \times W \times d}$
	\end{enumerate}
	
	\item[Parameters:] 
	\begin{enumerate}
		\item[]
		\item Interaction module $\cI$
		\item Attention module $\cA$
	\end{enumerate}
	
	\item[Outputs:] 
	\begin{enumerate}
		\item[]
		\item Visual object: $vo_t \in  \Reals^d$
		\item Visual attention: $va_t  \in \Reals^{H \times W \times d}$
	\end{enumerate}

	\item[Equations:]
	\begin{enumerate}
		\item[]
		\item Modified feature map: $\hat{F} = \cI(so_{t-1}, F)$
		\item Visual object and attention: $vo_t, va_t = \cA(y, \hat{F}, M_{t-1})$
	\end{enumerate}
\end{description}

\begin{note}
	Appropriate flatten/unflatten operations are performed to match the signature 
	of the modules.
\end{note}


\colorbx{Memory Retrieval Unit}

\begin{description}
	\item[Inputs:] 
	\begin{enumerate}
		\item[]
		\item Control state: $c_t \in \Reals^d$	
		\item Previous summary object: $so_{t-1} \in  \Reals^d$
		\item Previous VWM $M_{t-1} \in \Reals^{N \times d}$
	\end{enumerate}
	
	\item[Parameters:] 
	\begin{enumerate}
		\item[]
		\item Interaction module $\cI$
		\item Attention module $\cA$
	\end{enumerate}
	
	\item[Outputs:] 
	\begin{enumerate}
		\item[]
		\item Memory object: $mo_t \in \Reals^d$
		\item Read head: $\rhead_t \in \Reals^N$
	\end{enumerate}

	\item[Equations:]
	\begin{enumerate}
		\item[]
		\item Modified VWM: $\hat{M}_t = \cI(so_{t-1}, M_{t-1})$
		\item Memory object and attention: $mo_t, \rhead_t = \cA(y, \hat{M}_t, M_{t-1})$
	\end{enumerate}
\end{description}


\colorbx{Reasoning Unit}

%control_state, visual_object, memory_object, temporal_class_weights

\begin{description}
	\item[Inputs:] 
	\begin{enumerate}
		\item[]
		\item Control state: $c_t \in \Reals^d$
		\item Visual object: $vo_t \in \Reals^d$
		\item Memory object: $mo_t \in \Reals^d$
		\item Temporal class weights $\tau \in \Delta^3$
	\end{enumerate}
	
	\item[Parameters:] Validator modules $\cG, \cK: \Reals^{2d} \to \Reals$.
	Both $\cG, \cK$ are two-layer networks of $2d$ hidden units,
	using ELU activation in the hidden layer, and sigmoid in the output layer.
	
	\item[Output:] Predicate gates for the current reasoning step
	\begin{enumerate}
    	\item Object match predicate gates (i) image: $\imatch_t \in [0,1]$ and 
    	(ii) memory: $\mmatch_t \in [0,1]$.
		
		\item Memory update predicate gates (i) add: $\doadd_t \in [0,1]$ and
		(ii) replace: $\doreplace_t \in [0,1]$
    \end{enumerate}

	\item[Equations:]
	\begin{enumerate}
		\item[]
		\item $\imatch_t \in [0,1]$:
		It's true if there is a valid visual object. This assumes that
		the current reasoning step refers to either ```now''  or ``latest''.
		
		\item$\mmatch_t \in [0,1]$:
		It's true if there is a valid memory object. This assumes that
		the current reasoning step refers to either ``last'',  or alternatively ``latest'' 
		but there is no matching visual object.
		
		\item $\doadd_t$:
		
		
		\item $\doreplace_t$: 
\end{enumerate}
\end{description}


\colorbx{Memory Update Unit}
% visual_object, visual_working_memory, read_head, write_head, do_replace, do_add_new

\begin{description}
	\item[Inputs:] 
	\begin{enumerate}
		\item[]
		\item Visual object: $vo_t \in \Reals^d$
		\item Memory object: $mo_t \in \Reals^d$
		\item Memory update predicate gates:  $\doadd_t, \doreplace_t \in [0,1]$
		\item Read head: $\rhead_t \in \Reals^N$
		\item Previous VWM $M_{t-1} \in \Reals^{N \times d}$
		\item Previous write head: $\whead_{t-1} \in \Reals^N$
	\end{enumerate}

	\item[Outputs:] 
	\begin{enumerate}
		\item[]
		\item VWM $M_t \in \Reals^{N \times d}$
		\item Read head: $\rhead_t \in \Reals^N$
		\item Write head: $\whead_t \in \Reals^N$
	\end{enumerate}

\end{description}



\colorbx{Summary Object Update Unit}

\begin{description}
	\item[Inputs:] 
	\begin{enumerate}
		\item[]
		\item Previous summary object: $so_{t-1} \in \Reals^d$
		\item Visual object: $vo_t \in \Reals^d$
		\item Memory object: $mo_t \in \Reals^d$
		\item Object predicate gates: $\imatch_t, \mmatch_t \in [0,1]$
	\end{enumerate}
	
	\item[Parameters:] Concatenation linear layer $\cH$ 
	
	\item[Output:] 
     New summary object:
	             $so_t = \cH\bigl([so_{t-1}, (\imatch_t * vo_t + \mmatch_t * mo_t)]\bigr) \in \Reals^d$ 
	
\end{description}


\noindent\makebox[\linewidth]{\rule{\paperwidth}{1pt}}



\begin{figure}
	\includegraphics[width=\textwidth]{img/model2}
	\label{fig:model}
\end{figure}	

\begin{figure}
	\includegraphics[width=\textwidth]{img/image}
	\label{fig:model}
\end{figure}	

