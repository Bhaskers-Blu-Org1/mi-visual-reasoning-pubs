\section{Experiments}

\subsection{Generalization capabilities}

\subsubsection{CoGenT tasks}

We have reimplemented and trained the MAC model~\cite{hudson2018compositional} using MI-Prometheus~\cite{kornuta2018accelerating}, a framework based on PyTorch~\cite{paszke2017automatic}.
For all experiments, the training procedure is as follows: we train the MAC model for 20 epochs on 90\% of the training set of CoGenT. We keep the remaining 10\% for validating the model at every epoch, and use the original validation sets (CoGenT-A  \& -B) as test sets.
We used NVIDIA's GeForce GTX TITAN X GPUs. We followed the implementation details indicated in the supplemental material (Sec. A) of the original paper~\cite{hudson2018compositional}, to ensure a faithful comparison.


\begin{table}[htb]
	\centering
	\noindent
	\begin{adjustbox}{width=0.5\textwidth}
	\begin{tabular}{c c c c c c c }\toprule
		\multicolumn{1}{c }{\textbf{}} & \multicolumn{6}{c }{\textbf{Test Accuracy (\%)}} \\  
		\multicolumn{1}{c }{\textbf{}} & \multicolumn{6}{c }{on \texttt{valA} -- 150,000 samples} \\ 
		\cmidrule(lr){2-7}
		\multicolumn{1}{ c }{\textbf{Experiments}} & \textbf{Overall}& \textbf{Exist}  & \textbf{Count} & \textbf{CompareInteger} & \textbf{CompareAttribute} & \textbf{QueryAttribute}\\ 
		\cmidrule(lr){1-1}
		\cmidrule(lr){2-7}
		\multicolumn{1}{ c }{All tasks} & 95.32 & 98.4 	& 86.75	& 96.0	& 98.65	& 98.02 \\ 
		\cmidrule(lr){1-7}
		\multicolumn{1}{ c }{All tasks but Exist} & 90.33 	& 60.42	& 86.12	& 96.18	& 98.52 & 98.60 \\ 
		\multicolumn{1}{ c }{All tasks but Count} & 74.59 	& 97.51	& 0.0	& 94.37	& 98.42 & 98.47 \\ 
		\multicolumn{1}{ c }{All tasks but CompareInteger} & 91.53 	& 98.22	& 86.09	& 56.35	& 98.78 & 98.40 \\ 
		\multicolumn{1}{ c }{All tasks but CompareAttribute} & 81.92 	& 98.32	& 86.36	& 95.86	& 23.18 & 98.44 \\ 
		\multicolumn{1}{ c }{All tasks but QueryAttribute} & 42.17 	& 76.79	& 54.64	& 79.87 & 64.13 & 0.0 \\ 
		\cmidrule(lr){1-7}
		\multicolumn{1}{ c }{Exist only} & 26.07 & 74.20	& 0.0	& 59.79	& 59.15 & 0.0 \\ 
	    \multicolumn{1}{ c }{Count only} & 14.89  & 0.0	& 62.78	& 0.0 & 0.0 & 0.0 \\ 
	    \multicolumn{1}{ c }{CompareInteger only} & 23.44 & 48.15	& 0.0	& 77.98	& 54.08 & 0.0 \\ 
	    \multicolumn{1}{ c }{CompareAttribute only} & 23.50 & 51.93	& 0.0 & 59.06 & 61.73 & 0.0 \\ 
	\multicolumn{1}{ c }{QueryAttribute only} & 34.64 	& 0.0	& 0.0	& 0.0 & 0.0 & 97.08 \\ 
		\bottomrule
	\end{tabular}
    \end{adjustbox}
	\caption{CoGenT accuracies for all tasks when training on one task only or all but one task.}
	\label{tab:CoGenT_results}
\end{table}

We first ran a control experiment, where the model is trained, then tested on all tasks. We take the results as references for the following experiments. The accuracy per task is coherent with the human performance~\cite{johnson2017clevr}, where we observe that \textit{Count} is the most challenging task.

The next set of experiments excludes one task from the training set, and tests the model on all. Comparing these experiments against the reference one allows to observe whether the model is able to solve the held-out task using information from the other tasks.
We observe here a first distinction between the five tasks: 
\begin{itemize}
	\item \textit{Exist}, \textit{CompareInteger}, \textit{CompareAttribute}: Even with no training data for these tasks, the model is able to answer correctly the majority of the questions for \textit{Exist} \& \textit{CompareInteger} and one-quarter of the \textit{CompareAttribute} questions.
	\item \textit{Count} \& \textit{QueryAttribute}: When not present in the training data, the model cannot answer any of these questions correctly.
\end{itemize}

This first observation hints at a transfer of information between tasks -- to a limited degree. A subsequent set of experiments is then to train on only one task at a time and test on all. This will allow us to quantify what is the benefit provided by training on a certain task when solving others.

The last section of \tableref{tab:CoGenT_results} contains the results of this set of experiments.
We note the following:
\begin{itemize}
	\item A distinction between tasks is again observed: \{\textit{Exist}, \textit{CompareInteger}, \textit{CompareAttribute}\}, \{\textit{Count}\}, \{\textit{QueryAttribute}\},
	\item The tasks \{\textit{Exist}, \textit{CompareInteger}, \textit{CompareAttribute}\} seem to be benefiting from one another: When trained on one, the model is able to answer correctly the majority of questions in the two other tasks.
	\item \textit{QueryAttribute} and \textit{Count} are very distinct: the model is able to answer these questions correctly only when it was trained on them. Additionally, training solely on these tasks do not appear to benefit the other 3 tasks.
	\item Nevertheless, not including \textit{QueryAttribute} results in a test accuracy drop of 17 to 30+ pts on the other tasks. \textit{QueryAttribute} could thus be qualified as a "necessary but not sufficient" training task. This could be mitigated by the fact that \textit{QueryAttribute} is the largest task (by number of questions). Hence, removing it from the training set reduces the number of samples by a third.
\end{itemize}



