\section{Experiments}
\label{sec:experiments}
We implemented and trained our SAMNet model using MI-Prometheus~\cite{kornuta2018accelerating}, a framework based on Pytorch~\cite{paszke2017automatic}. We evaluated the model on the COG dataset~\cite{yang2018dataset}, a video reasoning~\cite{mogadala2019trends} dataset developed for the purpose of research on relational and temporal reasoning, as well as on the CLEVR dataset~\cite{johnson2017clevr}, created for Image Question Answering research.
Our experiments were designed to study SAMNet's performance as well as its generalization and transfer learning abilities in different settings.
%In some of our experiments, we also compare using a \emph{zero-shot learning} approach  for transfer learning to another approach
%involving \emph{finetuning}.
%Recall that the former involves evaluating the trained model of the source directly on the target while for the latter we do additional lightweight 
%training on the target labeled data before evaluating its performance. 

For the temporal and feature transfer experiments, we used two variants of both datasets.
For COG, an easy one (Canonical) and a Hard variant, differ mainly on the number of frames in the input sequence, the maximum amount of look-back in frame history with the relevant information for each frame, and number of distractors (see~\cref{tab:cog_variants}).
This experiment is described in~\cref{sec:temporal}.
\begin{table}[ht]
	\centering
		\begin{tabular}{lccc}
			\toprule
			Variant	& Frames & History	& Distractors \\ 
			\midrule
			Canonical & 4 & 3 & 1\\	
			Hard  & 8 & 7 & 10\\
			\bottomrule	
		\end{tabular}
	\caption{Details of the Canonical and Hard variants of COG.}
	\label{tab:cog_variants}
\end{table}\vspace{5pt}

For CLEVR, we consider the CoGenT (Constrained Generalization Test) variant which contains two conditions, differing on the combinations of attributes values.
\Cref{sec:feature} describes this experiment.
For the reasoning transfer experiments, we consider 23 classification task classes in COG and the 5 original question categories in CLEVR.
%More details on the composition of these datasets is available in Appendix \ref{sec:datasets-desc}.

\subsection{Performance of SAMNet vs baseline model~\cite{yang2018dataset}}
\label{sec:cog-baseline-compare}

We trained SAMNet using 8 reasoning steps and external memory of 8 address locations, each storing an array of 128 floats. 
%We have also carried out experiments with different numbers of reasoning steps and memory size, but this goes beyond the scope of this paper.
We compared our results with the baseline model introduced in the same paper as the COG dataset~\cite{yang2018dataset}.
The most important results are highlighted in~\cref{fig:samnet_cog_detailed}; full comparison can be found in the supplementary material.%Appendix~\ref{sec:cog-all-results}.

\begin{figure*}[htb]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=0.9\textwidth]{img/results/samnet_cog_orig_canonical_no_labels.png}
	\end{subfigure}%
	\newline
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=0.9\textwidth]{img/results/samnet_cog_orig_hard.png}
	\end{subfigure}%
	\caption{Comparison of test set accuracies of SAMNet (blue) with original results achieved by the 
		baseline model~\cite{yang2018dataset} (gray) on Canonical (top) and Hard (bottom) variants of the COG dataset.}
	\label{fig:samnet_cog_detailed}
\end{figure*}

For the Canonical variant (top row), we achieve similar accuracies for the majority of tasks (with a total average accuracy of 98.0\%, 
compared to 97.6\% for the baseline model), with significant improvements (around 13 points) for \textit{AndCompare} tasks.
As these tasks focus on compositional questions referring to two objects, we hypothesize that our model achieves better 
accuracy due to its ability to selectively pick and store relevant objects from the past frames in memory.
For the Hard variant, we achieve a total average accuracy of 96.1\% compared to 80.1\% for the baseline model, demonstrating
that our model can adapt to larger number of frames and distractors.
Despite there being some tasks in the Canonical variant where SAMNet achieved slightly lower accuracies,
% (between 0.2 and 1.8 points)
when comparing performances on the Hard variant, it improves upon the baseline model on all tasks, 
with improvements varying from 0.5 to more than 30 points, and especially on the more complex tasks in the dataset.

\subsection{Reasoning transfer on CLEVR and COG}
\label{sec:reasoning}
In these experiments using the CLEVR and COG datasets, we focus on analyzing the impact 
of each task relative to others using appropriate groupings of tasks.
%For each dataset, we defined appropriate task families that reflect the characteristics of that dataset and designed
%appropriate experiments to study the effectiveness of transfer learning across task families.

\subsubsection{CLEVR}
\label{sec:reasoning-clevr}
For the CLEVR dataset, we consider the question categories defined by the authors: \textit{Exist}, \textit{Count}, \textit{CompareInteger}, \textit{CompareAttribute}, \textit{QueryAttribute} and conducted the following experiments:
\begin{itemize}
	\compresslist
	\item Train and test SAMNet on a single task group $t$. These 5 experiments fit into the traditional ML setup of single-task learning;
	\item Train SAMNet on all task groups jointly and evaluate its performance on each task group $t$ separately.
	This is a transfer learning setting where for the source task family, the task is sampled from all questions,
	while for the target task family, the samples consist of questions from group $t$ only;
	\item Finally, for each task group $t$, we train SAMNet on all tasks but $t$, and test its performance on $t$.
	This can also be viewed as a transfer learning setting similar to the previous case.
	%These experiments steer away from single / multi-task training and rather fall under the Domain Adaptation setting, categorized as \emph{transductive} transfer learning in \cite{pan2009survey}.
\end{itemize}

Noticeable results are shown in~\cref{fig:CoGenT-results}, while the complete set is available in the supplementary material.%Appendix \ref{sec:full-cogent-results}.


Looking at~\cref{fig:CoGenT-results}, SAMNet does well on \textit{Count} and \textit{QueryAttribute}, but poorer on the 3 other tasks in the single-task learning setting (blue). Indeed, \textit{Exist}, \textit{CompareInteger} and \textit{CompareInteger} are binary tasks; \textit{Count} has for corresponding labels the digits 0 through 10 (a random agent would thus obtain $<$10\% accuracy) and \textit{QueryAttribute} maps to the set of object attributes values (15 labels).

Nevertheless, significant accuracy gains are noted when training jointly on all tasks (gray), ranging from 18 points to 37 points on 4 out of 5 tasks. \textit{QueryAttribute} only sees an increase of one point. One could qualify it as \textit{self-sufficient} as it does not appear to benefit from joint training with other tasks. These improvements suggest that related tasks benefit from joint training.
%We plan to run additional experiments to jointly train on $t$ and all possible subsets of the remaining 4 tasks ($2^4$ experiments per task). We hypothesize that the improvements on e.g. \textit{Exist} would mostly originate from training jointly on \textit{CompareAttribute} and \textit{CompareInteger} since all 3 tasks share the same output space (i.e., labels domain).

Finally, the ``all-tasks-but-$t$'' experiments (yellow) demonstrate that while tasks are related, one does not subsume another in terms of learning. Indeed, we can observe that for \textit{CompareAttribute}, while \textit{Exist} and \textit{CompareInteger} share the same output space, including them and holding out \textit{CompareAttribute} from the training set results in poor accuracy. We also observe no learning for \textit{Count} and \textit{QueryAttribute}, caused by these tasks having distinct labels domains, which do not overlap with the other tasks. Thus, if holding out samples from \textit{Count} during training, a model will not be able to predict the corresponding labels.

\begin{figure}[!t]
	\centering
	\includegraphics[width=\columnwidth]{img/results/CoGenT_results.pdf}
	\caption{CLEVR-CoGenT accuracies for all tasks $t$ when training on $t$ only, training on all tasks jointly and training on all tasks but $t$.} %For all experiments, the validation and test sets are identical.
	\label{fig:CoGenT-results}
\end{figure}

An additional set of experiments, for which results are available in the supplementary material,
%Appendix \ref{sec:full-cogent-results}
finetune the model trained on all tasks on each task $t$ respectively.
%Given the initial training on all tasks, we were interested in the tradeoff between the performance gain, if any, on the finetuned task and the drop, if any, on other tasks.
Finetuning did not demonstrate a clear benefit (except for \textit{Count}, where the accuracy increased by 1.5 pt) without hurting performance on the other tasks. Nevertheless, these experiments leave open the possibility that the multi-task learning training method may potentially benefit from using weighted sampling towards the tail end with more emphasis on samples from less performing task groups. Recent work~\cite{guo2018dynamic, kendall2018multi} in this direction has been done, although seems to weigh tasks rather than samples.

\subsubsection{COG}
\label{sec:reasoning-cog}
Since the number of task classes (=23) in the COG dataset is large, we designed a 2-level hierarchy of task groups using the
description of these tasks, as shown in~\cref{fig:task-groups}.

\begin{figure}[htb]
	\centering
	\includegraphics[width=\columnwidth]{img/architecture/hierarchy}
	\caption{Hierarchy of Task Groups.}
	\label{fig:task-groups}
\end{figure}

\bigskip

For groups at the lowest level, we chose the following task classes to be placed in those groups.
Below, substitute each of \textit{Shape} and \textit{Color} for  \uX{} to obtain the task class.
\begin{description}
	\compresslist
	\item[Basic:] \textit{Exist}\uX, \textit{Get}\uX{} and \textit{Exist};
	\item[Obj-Attr:] \emph{SimpleCompare}\uX{} and \textit{AndSimpleCompare}\uX;
	\item[Compare:] \textit{Compare}\uX,  \textit{AndCompare}\uX{} \& \textit{Exist}\uX\textit{Of};
	\item[Spatial:] \textit{ExistSpace}, \textit{Exist}\uX\textit{Space}, and \textit{Get}\uX\textit{Space};
	\item[Cognitive:] \textit{ExistLastColorSameShape}, \textit{ExistLastShapeSameColor} and \textit{ExistLastObjectSameObject}
\end{description}

We then conducted the following experiments using the Canonical variant of COG to study 
whether transfer learning was effective in leveraging information gained by training a task family at a higher level
of the hierarchy:
 \begin{itemize}
 	\compresslist
	\item Train and test SAMNet on each of the 5 task groups at the lowest level of the hierarchy (standard ML). 
	\item Train on \textbf{Group A} and test on \textbf{Basic}, \textbf{Obj-Attr} and \textbf{Compare} each;
	\item Train on \textbf{Group B} and test separately on \textbf{Spatial} and \textbf{Cognitive};
	\item As a baseline, we compared the above results to the earlier experiment shown in~\cref{fig:samnet_cog_detailed}, which
	can be viewed as training on \textbf{All} and testing on each group at the leaf level separately.
\end{itemize}

The results of these experiments are shown in~\cref{fig:COG-reasoning-results}.

First, notice that for each of the \textbf{Basic} and \textbf{Obj-Attr} task families, the accuracy is near-perfect in all cases, suggesting that each contains the most primitive of tasks and therefore do not benefit from training with other task families.
With \textbf{Spatial}, we see a small improvement showing that there is some benefit due to joint training with other task families.
Two groups that demonstrated a huge improvement of more than 25 points are \textbf{Compare} and \textbf{Cognitive}.
The former saw an accuracy jump from 68.6\% by training on samples from that family alone
to 97.0\% when training on all samples. To further emphasize this behavior, notice that 
just joining \textbf{Compare} with \textbf{Obj-Attr} and \textbf{Basic} already causes a significant accuracy jump to 92.3\%. 
In hindsight, this is not surprising, as the questions in \textbf{Compare} are composed
of fragments of questions given by \textbf{Basic} and \textbf{Obj-Attr}, and therefore can leverage the reasoning strategies developed there to reason about questions in \textbf{Compare}.
Lastly, for the \textbf{Spatial} family, we again see the benefits of joint training with all questions (68.6\% to 95.7\%) but in this case there
is a slight loss incurred by including everything. As seen in the figure, just jointly training with \textbf{Spatial} alone is sufficient to get a boost in accuracy (97.1\%). To summarize, while joint training helps, one needs to determine how much of correlation is present with the other tasks.

 \begin{figure}
 	\centering
 	\includegraphics[width=\columnwidth]{img/results/COG_reasoning_transfer.pdf}
 	\caption{COG accuracies for all task groups $t$ when training on $t$ only, training on Group A or B and training on all tasks.}
 	\label{fig:COG-reasoning-results}
 \end{figure}
 

\subsection{Temporal transfer in COG}
\label{sec:temporal}


The goal here is to test the transfer learning ability concerning the sequence length and number of distractors.
For that purpose, we compare both models when trained on the Canonical variant but tested on the 
Hard variant (\cref{fig:samnet_cog_overall_transfer}).

As the original paper does not include these experiments, we performed them. The light gray color indicates original results, whereas dark gray indicates the accuracies of COG models which we trained (fine-tuning/testing) using the original code provided by the authors.
For sanity check, in the first column, we report both the best-achieved score and the score reported in the paper when training and testing on the Canonical variant, without any fine-tuning. The observed close scores underline the faithfulness of the model reproducibility.
In a pure \textit{zero-shot learning} setup (second column), our model shows enormous generalization ability, reaching 91.6\% accuracy on the test set.
We also test both models in a setup where the model trained on a Canonical variant underwent additional fine-tuning (for a single epoch) on the Hard variant (third column).
In this case, the SAMNet model again reaches exemplary performance, at 96.7\% accuracy, and, interestingly, performs better than the model trained solely on the Hard variant.

\begin{figure}[htb]
	\centering
	\includegraphics[width=\columnwidth]{img/results/samnet_cog_overall_transfer.png}
	\caption{Total accuracies of SAMNet (blue) and COG models (light/dark gray) when testing generalization from Canonical to Hard variants of the dataset.}
	\label{fig:samnet_cog_overall_transfer}
\end{figure}


\subsection{Feature transfer on CLEVR-CoGenT}
\label{sec:feature}
\begin{table}[ht]
	\centering
	%\begin{adjustbox}{width=0.45\textwidth}
		\begin{tabular}{cccc}
			\toprule
			Dataset	& Cubes	& Cylinders &	Spheres	\\
			\midrule
			CoGenT A &  Family A  & Family B 	&	Any color  \\
			CoGenT B	&	Family B  &	Family A	&	Any color \\
			\bottomrule
		\end{tabular}
	%\end{adjustbox}
	\caption{Color restrictions in CoGenT-A \& -B datasets.}
	\label{tab:cogent_conditions}
\end{table}

The final set of experiments we consider is related to feature transfer.
The CoGenT-A \& -B sets of CLEVR differ by the available combinations of 8 object color attributes.
The colors are partitioned into two complementary families: 
Gray, Blue, Brown and Yellow in \textbf{Family A} and Red, Green, Purple, Cyan in \textbf{Family B}. 
The cubes and cylinders take colors from complementary families in each dataset with opposite configurations while
the spheres can take any color (See~\cref{tab:cogent_conditions}). 
Considering the input domain to be the set of objects with all attributes values, both sets differ by their marginal distributions $P_S$ and $P_T$. We consider 2 experiments with SAMNet trained on CoGenT-A: An immediate test (\emph{zero-shot learning}) from A to B and a finetuning of a single epoch on 30k Conditions B samples (following~\cite{johnson2017inferring, mascharka2018transparency, perez2018film, marois2018transfer}). The results are available in~\cref{fig:CoGenT-B-results}.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.8\columnwidth]{img/results/CoGenT_B_results.pdf}
	\caption{Test accuracy on CoGenT-A \& -B when training on CoGenT-A (blue) and finetuning on CoGenT-B (gray).}
	\label{fig:CoGenT-B-results}
\end{figure}
Evaluating SAMNet on CoGenT shows that performance is worse on CoGenT-B than CoGenT-A, in line with~\cite{johnson2017inferring, mascharka2018transparency, perez2018film}. Finetuning for a single epoch allows an observable increase of 15 pts on CoGenT-B, and a slight drop on CoGenT-A, also observed with other models.