\section{Introduction}
In recent years, neural networks, at the epicenter of the Deep Learning~\cite{lecun2015deep} revolution, became dominating solutions across many domains, from  Speech Recognition~\cite{graves2013speech}, Image Classification~\cite{krizhevsky2012imagenet}, Object Detection~\cite{redmon2016you}, to Question Answering~\cite{weston2014memory} and Machine Translation~\cite{bahdanau2014neural} among others.
At their core, being statistical models~\cite{ripley1993statistical,warner1996understanding}, neural networks rely on the assumption that training and testing samples are independent and identically distributed (\textit{iid}); i.e. sampled from a common input space under similar data distribution characteristics.
However, in many real-world scenarios, this assumption does not hold. Moreover, as modern neural models often have  millions of trainable parameters, training them requires vast amount of data, which for some domains (notably medical) can be very expensive and/or extremely difficult to collect.
One of the widely accepted solutions for the problems mentioned above is Transfer Learning~\cite{pan2009survey,weiss2016survey}, a technique which enables model - and thus performance - improvements by transferring \emph{information} from one domain to another.

%pan2009survey: we categorize transfer learning under three sub-settings: inductive transfer learning, transductive transfer learning and unsupervised transfer learning, 

In computer vision, it is now standard practice to pre-train an image encoder (such as VGG~\cite{simonyan2014very} or ResNet~\cite{he2016deep}) on large-scale datasets (such as ImageNet~\cite{deng2009imagenet}), and reuse the weights in unrelated domains and tasks, such as Visual Question Answering (VQA) in a medical domain~\cite{kornuta2019leveraging} or pixel-wise image segmentation of cars~\cite{iglovikov2018ternausnet}.
That such transfer improves performance is appealing, knowing that both the domain (e.g. realistic images vs. medical ones or images with cars) and the task (e.g. image classification vs. VQA or image segmentation) can change significantly. This type of transfer learning is qualified as \emph{inductive}, according to the taxonomy from~\cite{pan2009survey}.

A similar development can be noted in the Natural Language Processing (NLP) community.
Using shallow word embeddings, such as word2vec~\cite{mikolov2013distributed} or GloVe~\cite{pennington2014glove}, pretrained on large corpuses coming from e.g.\ Wikipedia or Twitter, became a standard procedure when working with different NLP domains and tasks, some where labeled data is less accessible.
Recently, we can denote an accelerated development of deep contextualized word representations such as ELMo (based on bidirectional LSTMs)~\cite{peters2018deep} or BERT (based on the Transformer~\cite{vaswani2017attention} architecture)~\cite{devlin2018bert}, which pretrain an entire network (and not just the input layer) on very large amount of data.
Analogically to the pretrained image encoders, the community is facilitating access to such pretrained models through repositories, some with dozens of pretrained BERT variations. HuggingFace~\cite{wolf2019transformers} is one of the most notable examples.

The success of transfer learning raises several questions, such as which characteristics make a dataset favorable for use in pretraining (notably ImageNet~\cite{huh2016makes}), or what explains the observed performance correlation of models with different architectures between the source and target domains~\cite{kornblith2019better}.
One of the most systematic work studying the impact of transfer learning in computer vision is the computational taxonomic map for task transfer learning~\cite{zamir2018taskonomy}, aiming at discovering the dependencies across a dictionary of twenty-six 2D, 2.5D, 3D, and semantic tasks.

The impact of transfer learning is even more central in the case of multi-modal tasks~\cite{mogadala2019trends}, combining both of the modalities mentioned above (i.e. vision and language).
One of the most popular problem domain is Visual Question Answering (VQA)~\cite{malinowski2014multi,antol2015vqa}, where the system is expected to answer natural language questions by taking into account the content of a single image (Image Question Answering) or several frames (Video Question Answering).
In a typical VQA model, setting aside whether it is performing an early fusion of embeddings of those two modalities~\cite{malinowski2018visual} or multi-step reasoning~\cite{hudson2018compositional}, using pretrained image encoders and word embeddings is de facto standard, having empirically proven its dominance over the models trained from the ground up.

In this work, we are focusing on a parallel aspect of transfer learning. As both Image and Video QA can be seen as meta-problems, where the model has to develop a different reasoning process depending on the type/family of question (task, e.g. counting objects or finding the value of a specific object's attribute), we asked ourselves whether mastering one task type/family can help with mastering others. This work is thus also deeply related to multi-task learning~\cite{caruana1997multitask}.

\paragraph{Contributions}

The contributions of this paper are as follows.
We propose a new model called SAMNet, that achieves state-of-the-art results on two datasets for Visual Reasoning, namely CLEVR~\cite{johnson2017clevr}, a diagnostic dataset for Image QA, and COG~\cite{yang2018dataset}, a Video QA reasoning.
Next we show how different tasks impact when transfering the whole pretrained model in three main setups.
In the first one focus on transfer learning within the Image QA domain and perform experiment similar to Taskonomy~\cite{zamir2018taskonomy} and analyse how training exclusively on one task family helps with mastering other family, with once but crucial difference: we transfer and fine-tune the whole model (not only the encoder).
In the second we show a complementary results in a multi-task learning setup, when the model is trained on all-but-one task and then tested on this task (with and without finetuning).
Finally, we present a set of experiments aimed at analysis of the impact of transfer learning in Video QA, when domain changes drastically and system has to deal with longer videos with higher complexity (understood as number of distractors present in each frame).
%Surprisingly, our model with only small fine-tuning, managed to the master Hard variant and achieved higher accuracy than the model trained explicitly on Hard variant from the scratch.

\tk{Have I addressed that correctly? I am not sure whether I have covered TL with CoGenT-A and CoGenT-B variants...}
%A) Task generalization
%- answers (CLEVR)
%B) Domain adaptation
%- CoGenT A -> CoGenT B (CLEVR)
%- sequence lenght  (COG)
%- image complexity (distractors) (COG)

%All those experiments in fact analyse different aspects 
 
%Lack of studies of impact of transfer learning for higher-level, abstract reasoning?

%From one point, some preliminary results:
% - CLEVR showing results on CoGenT-a/b~\cite{johnson2017clevr} - this is more "object generalization"
 %- COG paper showing interesting results with "task generalization" when training on all tasks excluding the one used for test~\cite{yang2018dataset}


%I want to claim that therer are three different approaches to "master count".
%1) Train and test only on count
%2) Train on alll the tasks including count
%3) Do not train on count at all and then test on count


%conclusions:
%when you are doing transfer make sure that the target answers are the same!
%otherwise: binary y/n vs numers 0-7 vs names of attributes



\section{Related work -- import}
%Integration of vision and language in deep neural network models allows the system to learn joint representations of objects, concepts, and relations.  Potentially, this approach can lead us towards Harnad's \textit{symbol grounding problem}~\cite{harnad2003symbol} but we are quite far from achieving the full capabilities of visually grounded language learning.

%Recently, there is a growing interest in neuro-symbolic approaches, which can combine the power of representation learning and symbolic logic that is interpretable \cite{mao2019neurosymbolic}. These approaches focus on \textit{symbol manipulation} rather than learning \textit{grounded symbols}.  Furthermore, symbolic priors (e.g., domain knowledge) and integration of logic depend on hand-crafted modules.  In the near term, this direction is certainly promising and can address some of the shortcomings of machine learning (i.e., the lack of explainability)\cite{vedantam2019probabilistic}.  However, in the long run, the desire is to learn grounded representations, which may lead to the emergence of symbols \cite{taniguchi2018symbol}.

Starting with Image Question Answering~\cite{malinowski2014multi,antol2015} and Image Captioning~\cite{karpathy2015deep}, a variety of tasks that integrate vision and language have appeared in the past several years~\cite{mogadala2019trends}. 
Those directions include e.g., Video QA~\cite{MovieQA} and Video Action Recognition~\cite{monfort2019moments}, that provide an additional challenge of understanding \emph{temporal} aspects, and Video Reasoning~\cite{song2018explore,yang2018dataset}, that tackles both spatial (comparison of object attributes, counting and other relational question) and temporal aspects and relations (e.g. object disappearance).
To deal with the temporal aspect most studies typically cut the whole video into clips; e.g., in~\cite{song2018explore} the model extracts visual features from each frame and aggregates features first into clips, followed by aggregation over clips to form a single video representation.
Still, when reasoning and producing the answer, the system in fact has \textit{access to all frames}. 
Similarly, in Visual Dialog~\cite{das2017visual} the system memorizes the whole dialog history.
However, in real-time dialog or video monitoring, it is not always possible to keep the entire history of conversation nor all frames from the beginning of the recording.  


As evident from human cognition, attention and memory are the key competencies required to solve these problems, and unsurprisingly, the AI research is rapidly growing in these areas.
The ability to deal with temporal casuality can pose a challenge for also in pure natural language processing (NLP) settings, e.g. in question answering (QA) and dialog applications.  
Current NLP solutions, in many problem settings, work around this challenge by processing the entire text input and reason over it multiple times using attention \cite{vaswani2017attention} or other mechanisms.
For example, typical solutions to the bAbI reasoning task, such as Memory Networks \cite{weston2014memory}, involve processing all the supporting facts at once and keeping them in memory all the time while searching for the answer.

