\section{Introduction}
%dummy
In recent years, neural networks, being at the epicenter of the Deep Learning~\cite{lecun2015deep} revolution, became the dominant solutions across many domains, from Speech Recognition~\cite{graves2013speech}, Image Classification~\cite{krizhevsky2012imagenet}, Object Detection~\cite{redmon2016you}, to Question Answering~\cite{weston2014memory} and Machine Translation~\cite{bahdanau2014neural} among others.
At their core, being statistical models~\cite{ripley1993statistical,warner1996understanding}, neural networks rely on the assumption that training and testing samples are independent and identically distributed (\textit{iid}); i.e.\ sampled from a common input space under similar data distribution characteristics.
However, in many real-world scenarios, this assumption does not hold. Moreover, as modern neural models often have millions of trainable parameters, training them requires vast amounts of data, which for some domains (e.g., medical) can be very expensive and/or extremely difficult to collect.
One of the widely used solutions for the above mentioned problems is Transfer Learning~\cite{pan2009survey,weiss2016survey}, a technique which enhances model performance by transferring \emph{information} from one domain to another.

%pan2009survey: we categorize transfer learning under three sub-settings: inductive transfer learning, transductive transfer learning and unsupervised transfer learning, 

In Computer Vision, it is now standard practice to pre-train an image encoder (such as VGG~\cite{simonyan2014very} or ResNet~\cite{he2016deep}) on large-scale datasets (such as ImageNet~\cite{deng2009imagenet}), and reuse the weights in unrelated domains and tasks, such as Visual Question Answering (VQA) in a medical domain~\cite{kornuta2019leveraging} or pixel-wise image segmentation of cars~\cite{iglovikov2018ternausnet}.
Such performance improvements are appealing, especially for cases where both the domain (e.g. realistic images vs. medical or cars ones) and the task (e.g. image classification vs. VQA or image segmentation) can change significantly. %This type of transfer learning is qualified as \emph{inductive}, according to the taxonomy from~\cite{pan2009survey}.

A similar development has emerged in the Natural Language Processing (NLP) community.
Using shallow word embeddings, such as word2vec~\cite{mikolov2013distributed} or GloVe~\cite{pennington2014glove}, pretrained on large corpuses coming from e.g.\ Wikipedia or Twitter, has become a standard procedure when working with different NLP domains and tasks, 
especially when labeled data is less accessible.
Recently, we observe a growing utilization of deep contextualized word representations such as ELMo (based on bidirectional LSTMs)~\cite{peters2018deep} or BERT (based on the Transformer~\cite{vaswani2017attention} architecture)~\cite{devlin2018bert}, which pretrain an entire network (and not just the input layer) on very large amounts of data.
In analogy to pretrained image encoders, the community is facilitating access to such pretrained models through repositories, some with dozens of pretrained BERT variations. HuggingFace~\cite{wolf2019transformers} is one of the most notable examples.

The success of transfer learning raises several questions for research, such as the characteristics which make a dataset more favorable to use in pretraining (notably ImageNet~\cite{huh2016makes}), or regarding the observed performance correlation of models with different architectures between the source and target domains~\cite{kornblith2019better}.
One of the most systematic work in this area is the computational taxonomic map for task transfer learning~\cite{zamir2018taskonomy}, which aim to discover the dependencies across a dictionary of twenty-six 2D, 2.5D, 3D, and semantic tasks.

In this work, we focus on multi-modal learning~\cite{mogadala2019trends} combining vision and language,
and in particular, transfer learning for visual reasoning tasks with a rich logical structure, e.g., see~\cite{johnson2017clevr,yang2018dataset}. 
While models such as BERT and ResNet can be transferred efficiently in the same modality they were pretrained on, challenges arise
once the modalities have been fused.
For example, in compositional reasoning problems that involve objects with multiple attributes,
feature representations in the source domain may be entangled and not easily transfer over to the target domain~\cite{johnson2017clevr}.
In video reasoning, an additional challenge in the temporal dimension is whether a model trained on shorter video sequences will 
transfer over to longer sequences~\cite{yang2018dataset,song2018explore}.
To address these challenges, mechanisms such as attention~\cite{bahdanau2014neural} 
and external memory~\cite{graves2014neural, graves2016hybrid,weston2014memory} 
which facilitate higher-level abstractions, seem more promising.

Our main contributions are:
\begin{itemize}
	\compresslist
	\item We propose a new model, called SAMNet (Selective Attention Memory Network), which achieves state-of-the-art results on COG~\cite{yang2018dataset}, a Video QA reasoning dataset.
	\item We propose a taxonomy of transfer learning, inspired from~\cite{pan2009survey}, applied to the domain of visual reasoning. Articulated around 3 main axes, we illustrate it through the COG dataset, as well as the CLEVR~\cite{johnson2017clevr} diagnostic dataset for Image QA.
	\item Following, we measure the impact of transferring the whole pretrained SAMNet model in the 3 proposed transfer learning settings: feature transfer, temporal transfer and reasoning transfer. This analysis is supported by an extensive set of experiments using the COG and CLEVR datasets, as well as their variants. Several of these experiments show the significant generalization capabilities of SAMNet.
\end{itemize}