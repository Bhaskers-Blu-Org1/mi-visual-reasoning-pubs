\section{Introduction}
In recent years, neural networks, being at the epicenter of the Deep Learning~\cite{lecun2015deep} revolution, became the dominant solutions across many domains, from Speech Recognition~\cite{graves2013speech}, Image Classification~\cite{krizhevsky2012imagenet}, Object Detection~\cite{redmon2016you}, to Question Answering~\cite{weston2014memory} and Machine Translation~\cite{bahdanau2014neural} among others.
At their core, being statistical models~\cite{ripley1993statistical,warner1996understanding}, neural networks rely on the assumption that training and testing samples are independent and identically distributed (\textit{iid}); i.e.\ sampled from a common input space under similar data distribution characteristics.
However, in many real-world scenarios, this assumption does not hold. Moreover, as modern neural models often have millions of trainable parameters, training them requires vast amounts of data, which for some domains (e.g., medical) can be very expensive and/or extremely difficult to collect.
One of the widely used solutions for the above mentioned problems is Transfer Learning~\cite{pan2009survey,weiss2016survey}, a technique which enhances the  model performance by transferring \emph{information} from one domain to another.

%pan2009survey: we categorize transfer learning under three sub-settings: inductive transfer learning, transductive transfer learning and unsupervised transfer learning, 

In Computer Vision, it is now a standard practice to pre-train an image encoder (such as VGG~\cite{simonyan2014very} or ResNet~\cite{he2016deep}) on large-scale datasets (such as ImageNet~\cite{deng2009imagenet}), and reuse the weights in unrelated domains and tasks, such as Visual Question Answering (VQA) in a medical domain~\cite{kornuta2019leveraging} or pixel-wise image segmentation of cars~\cite{iglovikov2018ternausnet}.
Such performance improvements are appealing, especially for cases where both the domain (e.g. realistic images vs. medical or cars ones) and the task (e.g. image classification vs. VQA or image segmentation) can change significantly. This type of transfer learning is qualified as \emph{inductive}, according to the taxonomy from~\cite{pan2009survey}.

A similar development has emerged in the Natural Language Processing (NLP) community.
Using shallow word embeddings, such as word2vec~\cite{mikolov2013distributed} or GloVe~\cite{pennington2014glove}, pretrained on large corpuses coming from e.g.\ Wikipedia or Twitter, became a standard procedure when working with different NLP domains and tasks, especially when labeled data is less accessible.
Recently, we observe a growing utilization of deep contextualized word representations such as ELMo (based on bidirectional LSTMs)~\cite{peters2018deep} or BERT (based on the Transformer~\cite{vaswani2017attention} architecture)~\cite{devlin2018bert}, which pretrain an entire network (and not just the input layer) on very large amounts of data.
In analogy to the pretrained image encoders, the community is facilitating access to such pretrained models through repositories, some with dozens of pretrained BERT variations. HuggingFace~\cite{wolf2019transformers} is one of the most notable examples.

The success of transfer learning raises several questions for research, such as the characteristics which make a dataset more favorable to use in pretraining (notably ImageNet~\cite{huh2016makes}), or regarding the observed performance correlation of models with different architectures between the source and target domains~\cite{kornblith2019better}.
One of the most systematic work in this area is the computational taxonomic map for task transfer learning~\cite{zamir2018taskonomy}, which aim to discover the dependencies across a dictionary of twenty-six 2D, 2.5D, 3D, and semantic tasks.

The impact of transfer learning is even more essential in the case of multi-modal tasks, \cite{mogadala2019trends}, which combine both modalities mentioned above (i.e. vision and language).
%One of the most popular problem domain is Visual Question Answering (VQA)~\cite{malinowski2014multi,antol2015vqa}, where the system is expected to answer natural language questions by taking into account the content of a single image (Image Question Answering) or several frames (Video Question Answering).
%In a typical VQA model, setting aside whether it is performing an early fusion of embeddings of those two modalities~\cite{malinowski2018visual} or multi-step reasoning~\cite{hudson2018compositional}, using pretrained image encoders and word embeddings is de facto standard, having empirically proven its dominance over the models trained from the ground up.
In this work, we focus on a parallel aspect of transfer learning. As both Image and Video QA can be seen as meta-problems, where the model has to develop a different reasoning process depending on the type/family of question (task, e.g. counting objects or finding the value of a specific object's attribute), we investigated whether mastering one task type/family can help to master others. Thus, this work is also deeply related to multi-task learning~\cite{caruana1997multitask}.

\paragraph{Contributions}

The contributions of this paper are as follows.
We propose a new model, called SAMNet (Selective Attention Memory Network), which achieves state-of-the-art results on two datasets for Visual Reasoning, namely CLEVR~\cite{johnson2017clevr}, a diagnostic dataset for Image QA, and COG~\cite{yang2018dataset}, a Video QA reasoning dataset.
We show the impact of the different tasks when transferring the whole pretrained model in three main setups.
\begin{itemize}
	\item We first focus on transfer learning within the Image QA domain and perform experiments similar to Taskonomy~\cite{zamir2018taskonomy}. We analyze how training exclusively on one task family helps with mastering the others, with one notable difference: we transfer and fine-tune the whole model (not only the encoder or input layer).
	\item Next, we show complementary results in a multi-task learning setup, when the model is trained on all-but-one task and then tested on this task (with and without finetuning).
	\item Finally, we present a set of experiments aimed at analyzing of the impact of transfer learning in Video QA, when domain changes drastically and the model is forced to handle longer videos with higher complexity (i.e. a higher number of distractor objects present in each frame).
\end{itemize}
Surprisingly, our model, with only small fine-tuning, managed to master the Hard variant of the COG dataset and achieved higher accuracy than the model trained solely on the Hard variant.

\tk{Have I addressed that correctly? I am not sure whether I have covered TL with CoGenT-A and CoGenT-B variants...}

\vm{I think the contributions should be: 1. A new model with SotA results on COG |  2. transfer  / multitask learning  in COG | 3.  transfer  / multitask learning  in CLEVR}.

%A) Task generalization
%- answers (CLEVR)
%B) Domain adaptation
%- CoGenT A -> CoGenT B (CLEVR)
%- sequence lenght  (COG)
%- image complexity (distractors) (COG)

%All those experiments in fact analyse different aspects 
 
%Lack of studies of impact of transfer learning for higher-level, abstract reasoning?

%From one point, some preliminary results:
% - CLEVR showing results on CoGenT-a/b~\cite{johnson2017clevr} - this is more "object generalization"
 %- COG paper showing interesting results with "task generalization" when training on all tasks excluding the one used for test~\cite{yang2018dataset}


%I want to claim that therer are three different approaches to "master count".
%1) Train and test only on count
%2) Train on alll the tasks including count
%3) Do not train on count at all and then test on count


%conclusions:
%when you are doing transfer make sure that the target answers are the same!
%otherwise: binary y/n vs numers 0-7 vs names of attributes
