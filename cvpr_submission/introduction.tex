\section{Introduction}
During recent year neural networks, being the core of the so-called Deep Learning~\cite{lecun2015deep} revolution, became dominating solutions across many domains, from  speech recognition~\cite{graves2013speech}, to image classification~\cite{krizhevsky2012imagenet} to object detection~\cite{redmon2016you} to question answering~\cite{weston2014memory} to machine translation~\cite{bahdanau2014neural}.
At their core, being statistical models~\cite{ripley1993statistical,warner1996understanding}, neural networks rely on the assumption that training and testing data are sampled from the same domain, i.e. operating in the same input space and with similar data distribution characteristics.
However, in many real-world scenarios, this assumption does not hold and besides, as modern neural models often have  milions of trainable parameters (weights), their trainng reguires a vaist amount of data, which for some domains (e.g. medical) can be very expensive and/or extremely difficult to collect.
One of the widely accepted solutions for the mentioned problems is Transfer Learning~\cite{pan2009survey,weiss2016survey}, a technique that enables improving of a model by transferring \emph{information} from one domain to the other. 

%pan2009survey: we categorize transfer learning under three sub-settings: inductive transfer learning, transductive transfer learning and unsupervised transfer learning, 

In Computer Vision, it became a standard to pretrain an image encoder (such as VGG~\cite{simonyan2014very}) on a large scale dataset (like ImageNet~\cite{deng2009imagenet}), and transfer its weights to unrelated domains and tasks, such as e.g. Visual Question Answering (VQA) in medical domain~\cite{kornuta2019leveraging} or pixel-wise image segmentation of cars~\cite{iglovikov2018ternausnet}.
The fact that such a transfer improves performance is quite fascinating taking into account that both domain (natural images vs medical or images with cars) and task (image classification vs VQA or image segmentation) change (inductive transfer learning according to the taxonomy from~\cite{pan2009survey}).

Similarly, NLP...

Analogically, the NLP community was using shallow word embeddings such as word2vec~\cite{mikolov2013distributed} or GloVe~\cite{pennington2014glove}.
The latter work came with a repository of pretrained models that became a standard for several years.
Recently, we can observe an explosion of deep contextualized word representations such as ELMo (based on bidirectional LSTM)~\cite{peters2018deep} or BERT (based on Transformer)~\cite{devlin2018bert}

Repositories with dozens of pretrained BERT variations such as HugginFace~\cite{wolf2019transformers} -- a new standard



The tremendous success of ImageNet-trained deep features on a wide range of transfer tasks raises the question:
what is it about the ImageNet dataset that makes the learnt
features as good as they are?~\cite{huh2016makes}

Transfer learning is a cornerstone of computer vision,
yet little work has been done to evaluate the relationship
between architecture and transfer~\cite{kornblith2019better}.
Here, we compare the performance of 16 classification networks on 12 image classification datasets. We find that, when networks are used as fixed feature extractors or fine-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy.

We proposes a fully computational approach for modeling the structure of space of visual tasks. This is done via
finding (first and higher-order) transfer learning dependencies across a dictionary of twenty six 2D, 2.5D, 3D, and
semantic tasks in a latent space. The product is a computational taxonomic map for task transfer learning~\cite{zamir2018taskonomy}



Visual Question Answering (VQA)~\cite{malinowski2014multi,antol2015vqa} is a new exciting problem domain, where the system is expected to answer questions expressed in natural language by taking into account the content of the image.

Conclusion:
TL for image, video(?) and language engoders are standard.
Are widely used for Visual Reasoning. But those tasks require some kind of fusion/higher level reasoning.
 
Lack of studies of impact of transfer learning for higher-level, abstract reasoning?

From one point, some preliminary results:
 - CLEVR showing results on CoGenT-a/b~\cite{johnson2017clevr} - this is more "object generalization"
 - COG paper showing interesting results with "task generalization" when training on all tasks excluding the one used for test~\cite{yang2018dataset}


\subsection{old}
Integration of vision and language in deep neural network models allows the system to learn joint representations of objects, concepts, and relations.  Potentially, this approach can lead us towards Harnad's \textit{symbol grounding problem}~\cite{harnad2003symbol} but we are quite far from achieving the full capabilities of visually grounded language learning.

Recently, there is a growing interest in neuro-symbolic approaches, which can combine the power of representation learning and symbolic logic that is interpretable \cite{mao2019neurosymbolic}. These approaches focus on \textit{symbol manipulation} rather than learning \textit{grounded symbols}.  Furthermore, symbolic priors (e.g., domain knowledge) and integration of logic depend on hand-crafted modules.  In the near term, this direction is certainly promising and can address some of the shortcomings of machine learning (i.e., the lack of explainability)\cite{vedantam2019probabilistic}.  However, in the long run, the desire is to learn grounded representations, which may lead to the emergence of symbols \cite{taniguchi2018symbol}.

Starting with Image Question Answering~\cite{malinowski2014multi,antol2015} and Image Captioning~\cite{karpathy2015deep}, a variety of tasks that integrate vision and language have appeared in the past several years~\cite{mogadala2019trends}. 
Those directions include e.g., Video QA~\cite{MovieQA} and Video Action Recognition~\cite{monfort2019moments}, that provide an additional challenge of understanding \emph{temporal} aspects, and Video Reasoning~\cite{song2018explore,yang2018dataset}, that tackles both spatial (comparison of object attributes, counting and other relational question) and temporal aspects and relations (e.g. object disappearance).
To deal with the temporal aspect most studies typically cut the whole video into clips; e.g., in~\cite{song2018explore} the model extracts visual features from each frame and aggregates features first into clips, followed by aggregation over clips to form a single video representation.
Still, when reasoning and producing the answer, the system in fact has \textit{access to all frames}. 
Similarly, in Visual Dialog~\cite{das2017visual} the system memorizes the whole dialog history.
However, in real-time dialog or video monitoring, it is not always possible to keep the entire history of conversation nor all frames from the beginning of the recording.  


As evident from human cognition, attention and memory are the key competencies required to solve these problems, and unsurprisingly, the AI research is rapidly growing in these areas.
The ability to deal with temporal casuality can pose a challenge for also in pure natural language processing (NLP) settings, e.g. in question answering (QA) and dialog applications.  
Current NLP solutions, in many problem settings, work around this challenge by processing the entire text input and reason over it multiple times using attention \cite{vaswani2017attention} or other mechanisms.
For example, typical solutions to the bAbI reasoning task, such as Memory Networks \cite{weston2014memory}, involve processing all the supporting facts at once and keeping them in memory all the time while searching for the answer.


\paragraph{Contributions}
