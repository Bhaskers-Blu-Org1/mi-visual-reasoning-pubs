\section{Related work}
Visual Question Answering (VQA) is a challenging multimodal task that combines vision and language.  Most image question answering datasets and tasks focus on identifying object attributes, counting, and reasoning about their spatial-relations. Prior work generally relied on dense visual features produced by either CNNs \cite{xiong2016dynamic,yang2016stacked} or object detection modules \cite{desta2018object}, and increasingly, recent models utilize the relationships among objects to augment those features with contextual information from each objectâ€™s surroundings \cite{teney2017graph,santoro2017simple}.  

Another research focus area has been multimodal fusion and
attention for VQA. For multimodal fusion, earlier methods used concatenation or element-wise multiplication between multimodal
features \cite{zhou2015simple,antol2015}.  Others proposed more sophisticated methods such as different approximated bilinear pooling methods to effectively integrate the multimodal features with second-order feature interactions \cite{fukui2016multimodal, kim2016hadamard, kim2016hadamard} .  Attention  research in VQA have demonstrated that learning question-guided visual attention on image regions is the most promising approach \cite{yang2016stacked, chen2015abc, ilievski2016focused}.   Compositional reasoning tasks such as the CLEVR dataset, also benefit from multi-hop attention and reasoning methods \cite{hudson2018compositional, song2018explore}.   Visualization of attention steps also provide interpretability, which is an increasingly important aspect. The MAC model \cite{hudson2018compositional} is a great example of such an approach.  The follow-on model by the same authors  \cite{hudson2019learning}, called Neural State Machine (NSM) takes a slightly different approach and reason over graph structures rather than directly over spatial maps of visual features.  Representing objects in a scene and their relationship as a graph is an obvious choice, which is another growing research direction \cite{haurilet2019s, teney2017graph, kim2018dynamic}. 

Video question answering, aside from spatial queries, also focuses on questions that require temporal reasoning .  Early works \cite{mun2017marioqa, xu2017video, yu2017end} used LSTMs to encode video frames and text to leverage temporal attention to attend to essential frames in a video selectively.   These approaches might be sufficient for action-recognition types of tasks but fall short when spatio-temporal reasoning is required.  Learning long-term dependencies is also another challenge that LSTMs may struggle with.  Yin et al. 2019 \cite{yin2019memory} recently proposed a memory-augmented neural network (MANN) architecture for video QA which leverages the external memory for storing and retrieving useful information in questions and videos and modeling the long-term visual-textual dependence.
Our work is mostly related to the MAC and MANN type models. An important distinction of our approach is the frame-by-frame processing of the video input.  Prior studies typically divide the whole video into clips; e.g., in~\cite{song2018explore} the model extracts visual features from each frame and aggregates features first into clips, followed by aggregation over clips to form a single video representation.
Nevertheless, when reasoning and producing the answer, the system has \textit{access to all frames}.  Similarly, in Visual Dialog~\cite{das2017visual} the system memorizes the whole dialog history. However, in real-time dialog or video monitoring, it is not always possible to keep the entire history of conversation nor all frames from the beginning of the recording. 